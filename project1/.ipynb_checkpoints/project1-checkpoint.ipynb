{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Project 1\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 153,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from collections import Counter"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Prostate Cancer Dataset\n",
    "\n",
    "Gene expression measurements for samples of prostate tumors and adjacent prostate tissue not containing tumor.\n",
    "\n",
    "Platform: Affymetrix Human Genome U95Av2 Array\n",
    "\n",
    "Diagnostic classes:\n",
    "* normal tissue (normal): 50 examples (49.0%)\n",
    "* prostate tumor (tumor): 52 examples (51.0%)\n",
    "\n",
    "Number of genes: 12533 <br>\n",
    "Number of samples: 102"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 154,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = pd.read_pickle('prostate.df')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div style=\"max-height:1000px;max-width:1500px;overflow:auto;\">\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>t</th>\n",
       "      <th>31308_at</th>\n",
       "      <th>31309_r_at</th>\n",
       "      <th>31310_at</th>\n",
       "      <th>31311_at</th>\n",
       "      <th>31312_at</th>\n",
       "      <th>31313_at</th>\n",
       "      <th>31314_at</th>\n",
       "      <th>31315_at</th>\n",
       "      <th>31316_at</th>\n",
       "      <th>...</th>\n",
       "      <th>101_at</th>\n",
       "      <th>102_at</th>\n",
       "      <th>103_at</th>\n",
       "      <th>104_at</th>\n",
       "      <th>105_at</th>\n",
       "      <th>106_at</th>\n",
       "      <th>107_at</th>\n",
       "      <th>108_g_at</th>\n",
       "      <th>109_at</th>\n",
       "      <th>class</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>-11.4</td>\n",
       "      <td>  2.7</td>\n",
       "      <td>  0.6</td>\n",
       "      <td>  4.3</td>\n",
       "      <td>  28</td>\n",
       "      <td>  0.3</td>\n",
       "      <td>-17.5</td>\n",
       "      <td> -5.4</td>\n",
       "      <td>  7.5</td>\n",
       "      <td> -0.9</td>\n",
       "      <td>...</td>\n",
       "      <td>   4.1</td>\n",
       "      <td>  2.5</td>\n",
       "      <td>  5</td>\n",
       "      <td> 22.5</td>\n",
       "      <td>  7.3</td>\n",
       "      <td> 14</td>\n",
       "      <td>  19.299999</td>\n",
       "      <td> -39.5</td>\n",
       "      <td> 37.299999</td>\n",
       "      <td> normal</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td> -1.0</td>\n",
       "      <td> -1.0</td>\n",
       "      <td>  0.0</td>\n",
       "      <td> -1.0</td>\n",
       "      <td>   3</td>\n",
       "      <td>  0.0</td>\n",
       "      <td> -3.0</td>\n",
       "      <td>  1.0</td>\n",
       "      <td> -2.0</td>\n",
       "      <td>  0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>  10.0</td>\n",
       "      <td>  2.0</td>\n",
       "      <td>  4</td>\n",
       "      <td>  5.0</td>\n",
       "      <td>  1.0</td>\n",
       "      <td>  6</td>\n",
       "      <td>   6.000000</td>\n",
       "      <td>   0.0</td>\n",
       "      <td> 26.000000</td>\n",
       "      <td> normal</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td> -9.0</td>\n",
       "      <td>-19.0</td>\n",
       "      <td>  0.0</td>\n",
       "      <td>  0.0</td>\n",
       "      <td>  76</td>\n",
       "      <td>  9.0</td>\n",
       "      <td>-57.0</td>\n",
       "      <td> 35.0</td>\n",
       "      <td> 23.0</td>\n",
       "      <td>  3.0</td>\n",
       "      <td>...</td>\n",
       "      <td> -66.0</td>\n",
       "      <td> 12.0</td>\n",
       "      <td> 14</td>\n",
       "      <td> 59.0</td>\n",
       "      <td> 16.0</td>\n",
       "      <td>-13</td>\n",
       "      <td> -10.000000</td>\n",
       "      <td>-156.0</td>\n",
       "      <td>-21.000000</td>\n",
       "      <td> normal</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>-16.0</td>\n",
       "      <td> -6.0</td>\n",
       "      <td>-16.0</td>\n",
       "      <td>  1.0</td>\n",
       "      <td>  67</td>\n",
       "      <td> -1.0</td>\n",
       "      <td>-57.0</td>\n",
       "      <td> -7.0</td>\n",
       "      <td>  6.0</td>\n",
       "      <td>  0.0</td>\n",
       "      <td>...</td>\n",
       "      <td> -14.0</td>\n",
       "      <td>  0.0</td>\n",
       "      <td> 23</td>\n",
       "      <td> 35.0</td>\n",
       "      <td>  5.0</td>\n",
       "      <td> 25</td>\n",
       "      <td> -27.000000</td>\n",
       "      <td>-103.0</td>\n",
       "      <td>  0.000000</td>\n",
       "      <td> normal</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td> -7.0</td>\n",
       "      <td>-17.0</td>\n",
       "      <td>  1.0</td>\n",
       "      <td> -3.0</td>\n",
       "      <td>  57</td>\n",
       "      <td>-18.0</td>\n",
       "      <td>-46.0</td>\n",
       "      <td>  8.0</td>\n",
       "      <td> 39.0</td>\n",
       "      <td>  0.0</td>\n",
       "      <td>...</td>\n",
       "      <td> -41.0</td>\n",
       "      <td> 16.0</td>\n",
       "      <td> 18</td>\n",
       "      <td> 49.0</td>\n",
       "      <td> 29.0</td>\n",
       "      <td> 32</td>\n",
       "      <td>  30.000000</td>\n",
       "      <td> -89.0</td>\n",
       "      <td>-13.000000</td>\n",
       "      <td> normal</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>-12.0</td>\n",
       "      <td>-13.0</td>\n",
       "      <td> -9.0</td>\n",
       "      <td> 15.0</td>\n",
       "      <td>  49</td>\n",
       "      <td> 10.0</td>\n",
       "      <td>-21.0</td>\n",
       "      <td> 24.0</td>\n",
       "      <td>  9.0</td>\n",
       "      <td>  3.0</td>\n",
       "      <td>...</td>\n",
       "      <td>  -7.0</td>\n",
       "      <td>  1.0</td>\n",
       "      <td> 14</td>\n",
       "      <td> 29.0</td>\n",
       "      <td>  2.0</td>\n",
       "      <td> 16</td>\n",
       "      <td> 113.000000</td>\n",
       "      <td>-112.0</td>\n",
       "      <td> 10.000000</td>\n",
       "      <td> normal</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>-21.0</td>\n",
       "      <td>-24.0</td>\n",
       "      <td>  6.0</td>\n",
       "      <td> -5.0</td>\n",
       "      <td>  83</td>\n",
       "      <td>  4.0</td>\n",
       "      <td>-83.0</td>\n",
       "      <td>-28.0</td>\n",
       "      <td> -4.0</td>\n",
       "      <td>-16.0</td>\n",
       "      <td>...</td>\n",
       "      <td> -23.0</td>\n",
       "      <td> 26.0</td>\n",
       "      <td> 14</td>\n",
       "      <td> 66.0</td>\n",
       "      <td> 11.0</td>\n",
       "      <td> -5</td>\n",
       "      <td>  22.000000</td>\n",
       "      <td>-146.0</td>\n",
       "      <td>-15.000000</td>\n",
       "      <td> normal</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td> -9.0</td>\n",
       "      <td> -6.0</td>\n",
       "      <td> -2.0</td>\n",
       "      <td>  1.0</td>\n",
       "      <td>  27</td>\n",
       "      <td>-30.0</td>\n",
       "      <td>-42.0</td>\n",
       "      <td>  7.0</td>\n",
       "      <td> 21.0</td>\n",
       "      <td> -4.0</td>\n",
       "      <td>...</td>\n",
       "      <td>   5.0</td>\n",
       "      <td> 16.0</td>\n",
       "      <td> 15</td>\n",
       "      <td> 26.0</td>\n",
       "      <td>  8.0</td>\n",
       "      <td> -4</td>\n",
       "      <td> -19.000000</td>\n",
       "      <td>-125.0</td>\n",
       "      <td> 15.000000</td>\n",
       "      <td> normal</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>-25.0</td>\n",
       "      <td>-16.0</td>\n",
       "      <td>-12.0</td>\n",
       "      <td>  0.0</td>\n",
       "      <td> 101</td>\n",
       "      <td> 18.0</td>\n",
       "      <td>-52.0</td>\n",
       "      <td> 26.0</td>\n",
       "      <td> 32.0</td>\n",
       "      <td>-10.0</td>\n",
       "      <td>...</td>\n",
       "      <td>-110.0</td>\n",
       "      <td> -1.0</td>\n",
       "      <td> 13</td>\n",
       "      <td> 75.0</td>\n",
       "      <td>  0.0</td>\n",
       "      <td> 27</td>\n",
       "      <td> 109.000000</td>\n",
       "      <td>-173.0</td>\n",
       "      <td> -3.000000</td>\n",
       "      <td> normal</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>-21.0</td>\n",
       "      <td>-11.0</td>\n",
       "      <td> -1.0</td>\n",
       "      <td> -2.0</td>\n",
       "      <td>  91</td>\n",
       "      <td>-40.0</td>\n",
       "      <td>-51.0</td>\n",
       "      <td> 20.0</td>\n",
       "      <td> 40.0</td>\n",
       "      <td>-12.0</td>\n",
       "      <td>...</td>\n",
       "      <td> -53.0</td>\n",
       "      <td>  3.0</td>\n",
       "      <td> 16</td>\n",
       "      <td> 73.0</td>\n",
       "      <td> 28.0</td>\n",
       "      <td> 24</td>\n",
       "      <td> -23.000000</td>\n",
       "      <td>-151.0</td>\n",
       "      <td>-14.000000</td>\n",
       "      <td> normal</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>10 rows Ã— 12534 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "      t  31308_at  31309_r_at  31310_at  31311_at  31312_at  31313_at  \\\n",
       "0 -11.4       2.7         0.6       4.3        28       0.3     -17.5   \n",
       "1  -1.0      -1.0         0.0      -1.0         3       0.0      -3.0   \n",
       "2  -9.0     -19.0         0.0       0.0        76       9.0     -57.0   \n",
       "3 -16.0      -6.0       -16.0       1.0        67      -1.0     -57.0   \n",
       "4  -7.0     -17.0         1.0      -3.0        57     -18.0     -46.0   \n",
       "5 -12.0     -13.0        -9.0      15.0        49      10.0     -21.0   \n",
       "6 -21.0     -24.0         6.0      -5.0        83       4.0     -83.0   \n",
       "7  -9.0      -6.0        -2.0       1.0        27     -30.0     -42.0   \n",
       "8 -25.0     -16.0       -12.0       0.0       101      18.0     -52.0   \n",
       "9 -21.0     -11.0        -1.0      -2.0        91     -40.0     -51.0   \n",
       "\n",
       "   31314_at  31315_at  31316_at    ...     101_at  102_at  103_at  104_at  \\\n",
       "0      -5.4       7.5      -0.9    ...        4.1     2.5       5    22.5   \n",
       "1       1.0      -2.0       0.0    ...       10.0     2.0       4     5.0   \n",
       "2      35.0      23.0       3.0    ...      -66.0    12.0      14    59.0   \n",
       "3      -7.0       6.0       0.0    ...      -14.0     0.0      23    35.0   \n",
       "4       8.0      39.0       0.0    ...      -41.0    16.0      18    49.0   \n",
       "5      24.0       9.0       3.0    ...       -7.0     1.0      14    29.0   \n",
       "6     -28.0      -4.0     -16.0    ...      -23.0    26.0      14    66.0   \n",
       "7       7.0      21.0      -4.0    ...        5.0    16.0      15    26.0   \n",
       "8      26.0      32.0     -10.0    ...     -110.0    -1.0      13    75.0   \n",
       "9      20.0      40.0     -12.0    ...      -53.0     3.0      16    73.0   \n",
       "\n",
       "   105_at  106_at      107_at  108_g_at     109_at   class  \n",
       "0     7.3      14   19.299999     -39.5  37.299999  normal  \n",
       "1     1.0       6    6.000000       0.0  26.000000  normal  \n",
       "2    16.0     -13  -10.000000    -156.0 -21.000000  normal  \n",
       "3     5.0      25  -27.000000    -103.0   0.000000  normal  \n",
       "4    29.0      32   30.000000     -89.0 -13.000000  normal  \n",
       "5     2.0      16  113.000000    -112.0  10.000000  normal  \n",
       "6    11.0      -5   22.000000    -146.0 -15.000000  normal  \n",
       "7     8.0      -4  -19.000000    -125.0  15.000000  normal  \n",
       "8     0.0      27  109.000000    -173.0  -3.000000  normal  \n",
       "9    28.0      24  -23.000000    -151.0 -14.000000  normal  \n",
       "\n",
       "[10 rows x 12534 columns]"
      ]
     },
     "execution_count": 119,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data.head(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Task 1\n",
    "\n",
    "(You can use DecisionTree implementation from scikit-learn.) \n",
    "\n",
    "Try decision tree on the above dataset. consider different values for the max depth of the tree ('max_depth') and min number of samples required to be a leaf node ('min-samples_leaf'). Conduct 10-fold cross-validation and: \n",
    "\n",
    "    - plot training error and testing error v.s. tree depth\n",
    "    - plot training error and testing error v.s. min. sample for leaf nodes\n",
    "    \n",
    "Error should be measured by percentage of misclassification (i.e., return 'normal' for 'tumor' and vice versa).    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from collections import Counter\n",
    "from sklearn import metrics\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn import cross_validation\n",
    "from sklearn.cross_validation import KFold \n",
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "from scipy.interpolate import spline\n",
    "import matplotlib\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = pd.read_pickle('prostate.df')\n",
    "cv=10\n",
    "X=data.values[:,:-1]\n",
    "Y=data.values[:,-1]\n",
    "kf = KFold(len(data), n_folds=cv)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def plot(train_errors, test_errors, xlabel, legendloc):\n",
    "    plt.figure()\n",
    "    h1 = plt.plot(max_depth_vals, train_errors,'b',label='Train Errors');\n",
    "    h2 = plt.plot(max_depth_vals, test_errors, 'r',label='Test Errors');\n",
    "    plt.legend(loc=legendloc)\n",
    "    plt.xlabel(xlabel)\n",
    "    plt.ylabel('Errors')\n",
    "    plt.title('Errors vs. Tree Depth')\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_treedepth(max_depth_vals):\n",
    "    train_errors = []\n",
    "    test_errors = []\n",
    "    for max_depth_val in max_depth_vals:\n",
    "        test_error=0\n",
    "        train_error=0\n",
    "        clf=DecisionTreeClassifier(max_depth=max_depth_val) \n",
    "        for train_index, test_index in kf:\n",
    "            train_x, test_x = X[train_index], X[test_index]\n",
    "            train_y, test_y = Y[train_index], Y[test_index] \n",
    "            clf = clf.fit(train_x, train_y)\n",
    "            pred_y=clf.predict(train_x)\n",
    "            train_accuracy=accuracy_score(train_y, pred_y)\n",
    "            train_error+=(1-train_accuracy)\n",
    "            pred_y=clf.predict(test_x)\n",
    "            test_accuracy=accuracy_score(test_y, pred_y)\n",
    "            test_error+=(1-test_accuracy)\n",
    "        train_errors.append(train_error/cv)\n",
    "        test_errors.append(test_error/cv)  \n",
    "    plot(train_errors, test_errors, 'Tree Depth','lower right')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_minsampleleaf(min_samples_leaf_vals):\n",
    "    train_errors=[]\n",
    "    test_errors=[]\n",
    "    for min_samples_leaf_val in min_samples_leaf_vals:\n",
    "        test_error=0\n",
    "        train_error=0\n",
    "        clf=DecisionTreeClassifier(min_samples_leaf=min_samples_leaf_val) \n",
    "        for train_index, test_index in kf:\n",
    "            train_x, test_x = X[train_index], X[test_index]\n",
    "            train_y, test_y = Y[train_index], Y[test_index] \n",
    "            clf = clf.fit(train_x, train_y)\n",
    "            pred_y=clf.predict(train_x)\n",
    "            train_accuracy=accuracy_score(train_y, pred_y)\n",
    "            train_error+=(1-train_accuracy)\n",
    "            pred_y=clf.predict(test_x)\n",
    "            test_accuracy=accuracy_score(test_y, pred_y)\n",
    "            test_error+=(1-test_accuracy)\n",
    "        train_errors.append(train_error/cv)\n",
    "        test_errors.append(test_error/cv)        \n",
    "    plot(train_errors, test_errors, 'Min Sample Leaf','upper right')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {},
   "outputs": [],
   "source": [
    "if __name__ == \"__main__\":\n",
    "    max_depth_vals=range(1,6)\n",
    "    min_samples_leaf_vals=range(1,6)\n",
    "    plot_treedepth(max_depth_vals)\n",
    "    plot_minsampleleaf(min_samples_leaf_vals)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Task 2\n",
    "\n",
    "Implement 2-class logistic regression using theano. <br>\n",
    "Use (stochastic) gradient descending to minimize negative loglikelihood of the data and obtain the value for the parameters. <br>\n",
    "(You must use the probability as defined on the lecture notes.) <br>\n",
    "(You cannot use any existing implementation of logistic regression or (stochastic) gradient descending. You must implement these yourself.)\n",
    "\n",
    "    - plot negative loglikelihood v.s. iteration for your SGD.\n",
    "\n",
    "\n",
    "Investigate how different starting values for the parameters affect the final negative loglikelihood. \n",
    "\n",
    "    - report the final neg. loglike. for 1) all parameters are initialized to be zero; 2) they are initialized to random values within a range of your choice.  \n",
    "\n",
    "Conduct 10-fold cross-validation, \n",
    "\n",
    "    - report the training error and the test error. \n",
    "    \n",
    "Add L1-regularization to your model, i.e., the optimization should minimize \n",
    "\n",
    "$$nll + c*\\sum_i |W_i|$$\n",
    "\n",
    "where nll is the neg. log likelihood and W is the parameter vector (except the bias). Try different values for c\n",
    "and conduct 10-fold cross-validation. Plot:\n",
    "\n",
    "    - number of non-zero parameters v.s. c\n",
    "    - 10-fold cross-validation test error v.s. c "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import math\n",
    "import numpy\n",
    "\n",
    "import theano\n",
    "import theano.tensor as T\n",
    "\n",
    "from sklearn.cross_validation import KFold \n",
    "\n",
    "from scipy.interpolate import spline\n",
    "import matplotlib\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.metrics import accuracy_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def appendrow(array):\n",
    "    array=array.transpose()\n",
    "    row=array.shape[0]\n",
    "    col=array.shape[1]\n",
    "    array2=np.zeros((row+1,col))\n",
    "    for i in range(0,row):\n",
    "        array2[i]=array[i]\n",
    "    array2[row]=[1]*col\n",
    "    return array2.transpose()\n",
    "\n",
    "def sigmoid(x):\n",
    "    return 1 / (1 + np.exp(-x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class LogisticRegression:\n",
    "    def __init__(self,n_attributes,learning_rate=1e-10, iterations=1000):\n",
    "        self.w=np.zeros(n_attributes+1)\n",
    "        self.learning_rate=learning_rate\n",
    "        self.iterations=iterations\n",
    "    def set_w(self, w):\n",
    "        self.w=w    \n",
    "    def set_iterations(self, iterations):\n",
    "        self.iterations=iterations\n",
    "    def set_learning_rate(self, learning_rate):\n",
    "        self.learning_rate=learning_rate    \n",
    "    def error(self, train_x,train_y):\n",
    "        er=0.0\n",
    "        for i in range(0,train_y.shape[0]):\n",
    "            pred_y=sigmoid(np.dot(self.w,train_x[i]))\n",
    "            if pred_y >0.5:\n",
    "                pred_y=1\n",
    "            else:\n",
    "                pred_y=0\n",
    "            if train_y[i]!=pred_y:\n",
    "                er+=1\n",
    "        return er/train_y.shape[0]\n",
    "    #   return accuracy_score(train_y, self.w*train_x)\n",
    "    def cost(self, train_x,train_y):\n",
    "        error=0\n",
    "        for i in range(0,train_y.shape[0]):\n",
    "            yi=train_y[i]\n",
    "            xi=train_x[i]\n",
    "            tmp=sigmoid(numpy.dot(self.w,xi))\n",
    "            if yi==1:\n",
    "                error-=np.log(tmp)\n",
    "            if yi==0:\n",
    "                error-=np.log(1-tmp)\n",
    "        return error/train_y.shape[0]     \n",
    "    def train_process(self, data_set_x, data_set_y):\n",
    "        #w=(wt,b)\n",
    "        n_samples=data_set_y.shape[0]\n",
    "        data_x=appendrow(data_set_x)\n",
    "        data_y=data_set_y\n",
    "        errors=[]\n",
    "        for iteration in range(0,self.iterations):\n",
    "            gd=numpy.zeros(data_set_x.shape[1]+1)\n",
    "            for i in range(0,n_samples):\n",
    "                xi=data_x[i] \n",
    "                yi=data_y[i]\n",
    "                w_rate=sigmoid(numpy.dot(self.w,xi))-yi\n",
    "                gd+=w_rate*xi\n",
    "            self.w-=self.learning_rate*gd\n",
    "            error=self.cost(data_x,data_y)\n",
    "            errors.append(error)\n",
    "        return errors   \n",
    "    def train_error(self, data_set_x, data_set_y):\n",
    "        data_x=appendrow(data_set_x)\n",
    "        data_y=data_set_y\n",
    "        return self.error(data_x,data_y)  \n",
    "    def test_error(self, data_set_x, data_set_y):\n",
    "        data_x=appendrow(data_set_x)\n",
    "        data_y=data_set_y\n",
    "        return self.error(data_x,data_y)               "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plotx():\n",
    "    data = pd.read_pickle('prostate.df')\n",
    "    X=(data.values[:,:-1]).astype(float) \n",
    "    Y=(data.values[:,-1]=='normal').astype(int)   \n",
    "    iterations=1000\n",
    "    learning_rate=1e-10\n",
    "    n_attributes=X.shape[1]\n",
    "    lr=LogisticRegression(n_attributes, learning_rate, iterations)   \n",
    "    train_errors=lr.train_process(X, Y) \n",
    "    \n",
    "    plt.figure()\n",
    "    h = plt.plot(range(0,iterations), train_errors,'b',label='Negative Loglikelihood');\n",
    "    plt.legend(loc='upper right')\n",
    "    plt.xlabel('Iterations')\n",
    "    plt.ylabel('Negative Loglikelihood')\n",
    "    plt.title('Negative Loglikelihood vs. Iteration Times')\n",
    "    plt.show()\n",
    "plotx()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "def different_parameter():\n",
    "    data = pd.read_pickle('prostate.df')\n",
    "    X=(data.values[:,:-1]).astype(float) \n",
    "    Y=(data.values[:,-1]=='normal').astype(int)   \n",
    "    iterations=1000\n",
    "    learning_rate=1e-10\n",
    "    n_attributes=X.shape[1]\n",
    "    lr=LogisticRegression(n_attributes, learning_rate, iterations) \n",
    "    print \"all parameters are zeros:\"\n",
    "    w=np.zeros(n_attributes+1)\n",
    "    lr.set_w(w)\n",
    "    print \"initial w: \" \n",
    "    print lr.w\n",
    "    lr.train_process(X,Y)\n",
    "    print \"final w: \"\n",
    "    print lr.w\n",
    "    error=lr.train_error(X,Y)\n",
    "    print \"error: \"+str(lr.train_error(X,Y))\n",
    "    print \"\"\n",
    "    print \"the parameters are initialized to random values: \"\n",
    "    for i in range(0,2):\n",
    "        print \"the \"+str(i)+\" random parameters:\"\n",
    "        w=1e-4*np.random.random_sample(n_attributes+1)-1e-4 \n",
    "        lr.set_w(w)\n",
    "        print \"initial w: \"\n",
    "        print w\n",
    "        lr.train_process(X,Y)\n",
    "        print \"final w:\"\n",
    "        print lr.w\n",
    "        error=lr.train_error(X,Y)\n",
    "        print \"error: \"+str(lr.train_error(X,Y))\n",
    "        print \"\"\n",
    "different_parameter()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "train error: \n",
      "0.0272336359293\n",
      "test error: \n",
      "0.165454545455\n",
      "\n"
     ]
    }
   ],
   "source": [
    "def errors_ten_fold_cross_validation():\n",
    "    data = pd.read_pickle('prostate.df')\n",
    "    X=(data.values[:,:-1]).astype(float) \n",
    "    Y=(data.values[:,-1]=='normal').astype(int)   \n",
    "    iterations=1000\n",
    "    learning_rate=1e-10\n",
    "    n_attributes=X.shape[1]\n",
    "\n",
    "    cv=10\n",
    "    kf = KFold(len(data), n_folds=cv)\n",
    "    train_errors=0\n",
    "    test_errors=0\n",
    "    for train_index, test_index in kf:\n",
    "            train_x, test_x = X[train_index], X[test_index]\n",
    "            train_y, test_y = Y[train_index], Y[test_index]            \n",
    "            lr=LogisticRegression(X.shape[1], learning_rate, iterations)   \n",
    "            lr.train_process(train_x,train_y)\n",
    "            train_errors+=lr.train_error(train_x, train_y)\n",
    "            test_errors+=lr.test_error(test_x, test_y)\n",
    "    train_errors/=cv\n",
    "    test_errors/=cv\n",
    "    print \"\"\n",
    "    print \"train error: \"\n",
    "    print train_errors\n",
    "    print \"test error: \"\n",
    "    print test_errors\n",
    "    print \"\"\n",
    "errors_ten_fold_cross_validation()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "class LogisticRegression_L1R:\n",
    "    def __init__(self,n_attributes,learning_rate=1e-14, c=0, iterations=1000):\n",
    "        self.w=np.zeros(n_attributes+1)\n",
    "        self.learning_rate=learning_rate\n",
    "        self.c=c\n",
    "        self.iterations=iterations\n",
    "    def set_w(self, w):\n",
    "        self.w=w    \n",
    "    def set_c(self, c):\n",
    "        self.c=c     \n",
    "    def set_iterations(self, iterations):\n",
    "        self.iterations=iterations\n",
    "    def set_learning_rate(self, learning_rate):\n",
    "        self.learning_rate=learning_rate   \n",
    "    def error(self, train_x,train_y):\n",
    "        er=0.0\n",
    "        for i in range(0,train_y.shape[0]):\n",
    "            pred_y=sigmoid(np.dot(self.w,train_x[i]))\n",
    "            if pred_y >0.5:\n",
    "                pred_y=1\n",
    "            else:\n",
    "                pred_y=0\n",
    "            if train_y[i]!=pred_y:\n",
    "                er+=1\n",
    "        return er/train_y.shape[0]\n",
    "    def stat_non_zero(self):\n",
    "        n=0\n",
    "        for i in range(0,len(self.w)-1):\n",
    "            if abs(self.w[i])>1e-5:\n",
    "                n+=1\n",
    "        return n\n",
    "    def loglikehood(self, train_x,train_y):\n",
    "        error=0\n",
    "        for i in range(0,train_y.shape[0]):\n",
    "            yi=train_y[i]\n",
    "            xi=train_x[i]\n",
    "            tmp=sigmoid(np.dot(self.w,xi))\n",
    "            if yi==1:\n",
    "                error-=np.log(tmp)\n",
    "            if yi==0:\n",
    "                error-=np.log(1-tmp)\n",
    "        return error     \n",
    "    def cost(self, train_x, train_y):\n",
    "        error=self.loglikehood(train_x, train_y)\n",
    "        wsum=0\n",
    "        for i in range(0,len(self.w)-1):\n",
    "            wsum+=abs(self.w[i])\n",
    "        return error+self.c*wsum\n",
    "    def train_process(self, data_set_x, data_set_y):\n",
    "        n_samples=data_set_y.shape[0]\n",
    "        data_x=appendrow(data_set_x)\n",
    "        data_y=data_set_y\n",
    "        error=float('inf')\n",
    "        w=None\n",
    "        for eps in range(0, self.iterations):\n",
    "            gd=np.zeros(data_set_x.shape[1]+1)\n",
    "            for i in range(0,n_samples):\n",
    "                xi=data_x[i] \n",
    "                yi=data_y[i]\n",
    "                w_rate=sigmoid(numpy.dot(self.w,xi))-yi\n",
    "                gd+=w_rate*xi\n",
    "            self.w-=self.learning_rate*gd\n",
    "            error_tmp=self.cost(data_x,data_y)\n",
    "#           print \"train_process error: \"\n",
    "#           print error\n",
    "            if error > error_tmp :\n",
    "                error = error_tmp\n",
    "                w=self.w\n",
    "        self.w=w\n",
    "    def train_error(self, data_set_x, data_set_y):\n",
    "        data_x=appendrow(data_set_x)\n",
    "        data_y=data_set_y\n",
    "        return self.error(data_x,data_y)  \n",
    "    def test_error(self, data_set_x, data_set_y):\n",
    "        data_x=appendrow(data_set_x)\n",
    "        data_y=data_set_y\n",
    "        return self.error(data_x,data_y)          "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 130,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[230, 400, 531, 671, 833, 973, 1119]\n"
     ]
    }
   ],
   "source": [
    "#- number of non-zero parameters v.s. c\n",
    "#  here consider the parameter with absolute value smaller than 1e-5 as zero\n",
    "def plot_non_zero():\n",
    "    data = pd.read_pickle('prostate.df')\n",
    "    X=(data.values[:,:-1]).astype(float) \n",
    "    Y=(data.values[:,-1]=='normal').astype(int)   \n",
    "    learning_rate=1e-10\n",
    "    iterations=1000  \n",
    "    n_attributes=X.shape[1]\n",
    "    lr=LogisticRegression_L1R(n_attributes, learning_rate, 0, iterations) \n",
    "    nonzeros=[]\n",
    "    cs=[1e6,2e6,3e6,4e6,5e6]\n",
    "    \n",
    "    for c in cs:\n",
    "        lr.set_c(c)\n",
    "        lr.train_process(X,Y)\n",
    "#        print lr.w\n",
    "        nonzeros.append(lr.stat_non_zero())\n",
    "#        nonzeros.append(lr.train_error(X,Y))\n",
    "    print nonzeros\n",
    "    plt.figure()\n",
    "    h = plt.plot(cs, nonzeros,'b',label='number of non-zero parameters');\n",
    "    plt.legend(loc='lower right')\n",
    "    plt.xlabel('c')\n",
    "    plt.ylabel('number of non-zero parameters')\n",
    "    plt.title('number of non-zero parameters vs. c')\n",
    "    plt.show()\n",
    "plot_non_zero()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 10-fold cross-validation test error v.s. c \n",
    "def test_error_ten_fold_cross_validation():\n",
    "    data = pd.read_pickle('prostate.df')\n",
    "    X=(data.values[:,:-1]).astype(float) \n",
    "    Y=(data.values[:,-1]=='normal').astype(int)   \n",
    "    iterations=1000  \n",
    "    n_attributes=X.shape[1]\n",
    "    learning_rate=1e-10\n",
    "\n",
    "    cv=10\n",
    "    kf = KFold(len(data), n_folds=cv)\n",
    "    test_errors=[]\n",
    "    cs=[1e6,2e6,3e6,4e6,5e6]\n",
    "    for c in cs:\n",
    "        error=0\n",
    "        for train_index, test_index in kf:\n",
    "            train_x, test_x = X[train_index], X[test_index]\n",
    "            train_y, test_y = Y[train_index], Y[test_index]            \n",
    "            lr=LogisticRegression_L1R(X.shape[1], learning_rate, c, iterations)   \n",
    "            lr.train_process(train_x,train_y)\n",
    "            error+=lr.test_error(test_x, test_y)\n",
    "        test_errors.append(error/cv)\n",
    "        \n",
    "    plt.figure()\n",
    "    h = plt.plot(cs, test_errors,'b',label='test error');\n",
    "    plt.legend(loc='upper right')\n",
    "    plt.xlabel('c')\n",
    "    plt.ylabel('test error')\n",
    "    plt.title('test error vs. c')\n",
    "    plt.show()\n",
    "test_error_ten_fold_cross_validation()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
