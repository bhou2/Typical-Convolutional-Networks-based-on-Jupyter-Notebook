{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Project 2\n",
    "\n",
    "\n",
    "### Task 1\n",
    "Train the CNN (as much as you can) to reach convergence. Investigate what patterns the first layer (layer 0) filters pick up by plotting the filters as small 2d images. To plot a 2d array x as image, use \"imshow(x, cmap=cm.gray)\". You should plot the 10 filters together using subplot. \n",
    "\n",
    "### Task 2\n",
    "The given CNN has 2 conv&pool layers, 1 hidden layer and 1 output layer. \n",
    "Modify the CNN to have:\n",
    "  - 1 conv&pool layer, 1 hidden layer and 1 output layer;\n",
    "  - 1 hidden layer and 1 output layer; \n",
    "  \n",
    "while keeping the other parameters the same. Compare the error rates on the test data for the original CNN and the two modifications and determine whether the conv&pool layers play a significant role for performance.\n",
    "\n",
    "### Task 3\n",
    "Change the number of filters for the two conv&pool layers:\n",
    "  - try 10 filters for layer 1 and 20 for layer 2;\n",
    "  - try 20 filters for layer 1 and 10 for layer 2.\n",
    "\n",
    "Compare error rate of the two cases and that of the original. Comment on how number of filters can impact performance. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Populating the interactive namespace from numpy and matplotlib\n"
     ]
    }
   ],
   "source": [
    "%pylab inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "ename": "ImportError",
     "evalue": "cannot import name downsample",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mImportError\u001b[0m                               Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-8-412c23361257>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      6\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mtheano\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      7\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mtheano\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtensor\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mT\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 8\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0mtheano\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtensor\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msignal\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mdownsample\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      9\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mtheano\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtensor\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnnet\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mconv\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mImportError\u001b[0m: cannot import name downsample"
     ]
    }
   ],
   "source": [
    "import cPickle, gzip\n",
    "\n",
    "import numpy\n",
    "import time\n",
    "\n",
    "import theano\n",
    "import theano.tensor as T\n",
    "from theano.tensor.signal import downsample\n",
    "from theano.tensor.nnet import conv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "class HiddenLayer(object):\n",
    "\n",
    "    def __init__(self, input, n_in, n_out):\n",
    "        self.input = input\n",
    "\n",
    "        W_values = 4*numpy.random.uniform(\n",
    "                low=-numpy.sqrt(6. / (n_in + n_out)),\n",
    "                high=numpy.sqrt(6. / (n_in + n_out)),\n",
    "                size=(n_in, n_out)\n",
    "        )\n",
    "        self.W = theano.shared(value=W_values, name='W', borrow=True)\n",
    "        self.b = theano.shared(value=numpy.zeros((n_out,)), name='b', borrow=True)\n",
    "        self.params = [self.W, self.b]\n",
    "\n",
    "        net = T.dot(self.input, self.W) + self.b\n",
    "        self.output = T.nnet.sigmoid(net)\n",
    "\n",
    "        \n",
    "class MultiLogisticRegression(object):\n",
    "\n",
    "    def __init__(self, input, n_in, n_out):\n",
    "\n",
    "        self.input = input\n",
    "\n",
    "        self.W = theano.shared(\n",
    "            value=numpy.zeros((n_in, n_out)),\n",
    "            name='W',\n",
    "            borrow=True\n",
    "        )\n",
    "        self.b = theano.shared(\n",
    "            value = numpy.zeros((n_out,)),\n",
    "            name='b',\n",
    "            borrow=True\n",
    "        )\n",
    "        self.params = [self.W, self.b]\n",
    "\n",
    "        self.prob = T.nnet.softmax(T.dot(self.input, self.W) + self.b)\n",
    "        self.predict = T.argmax(self.prob, axis=1)\n",
    "\n",
    "    def nll(self, y):\n",
    "        return  -T.mean(T.log(self.prob)[T.arange(y.shape[0]), y])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ConvPoolLayer(object):\n",
    "\n",
    "    def __init__(self, input, filter_shape, image_shape, poolsize):\n",
    "\n",
    "        self.input = input\n",
    "\n",
    "        n_in = filter_shape[1]*filter_shape[2]*filter_shape[3]\n",
    "        n_out = (filter_shape[0]*filter_shape[2]*filter_shape[3])/(poolsize[0]*poolsize[1])\n",
    "        W_bound = numpy.sqrt(6./(n_in + n_out))\n",
    "        self.W = theano.shared(\n",
    "            numpy.random.uniform(low=-W_bound, high=W_bound, size=filter_shape),\n",
    "            borrow=True\n",
    "        )\n",
    "        self.b = theano.shared(value=numpy.zeros((filter_shape[0],)), borrow=True)\n",
    "        self.params = [self.W, self.b]\n",
    "        \n",
    "        conv_out = conv.conv2d(\n",
    "            input=self.input,\n",
    "            filters=self.W,\n",
    "            filter_shape=filter_shape,\n",
    "            image_shape=image_shape\n",
    "        )\n",
    "\n",
    "        pooled_out = downsample.max_pool_2d(\n",
    "            input=conv_out,\n",
    "            ds=poolsize,\n",
    "            ignore_border=True\n",
    "        )\n",
    "\n",
    "        self.output = T.tanh(pooled_out + self.b.dimshuffle('x', 0, 'x', 'x'))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def ConvNN_Origin(num_epochs):\n",
    "    batch_size = 250\n",
    "    learning_rate=0.1\n",
    "    nkerns=[10, 10]\n",
    "\n",
    "\n",
    "    x = T.matrix('x')\n",
    "    y = T.ivector('y')\n",
    "\n",
    "\n",
    "    layer0_input = x.reshape((batch_size, 1, 28, 28))\n",
    "    layer0 = ConvPoolLayer(\n",
    "        input=layer0_input,\n",
    "        image_shape=(batch_size, 1, 28, 28),\n",
    "        filter_shape=(nkerns[0], 1, 5, 5),\n",
    "        poolsize=(2, 2)\n",
    "    )\n",
    "\n",
    "    layer1 = ConvPoolLayer(\n",
    "        input=layer0.output,\n",
    "        image_shape=(batch_size, nkerns[0], 12, 12),\n",
    "        filter_shape=(nkerns[1], nkerns[0], 5, 5),\n",
    "        poolsize=(2, 2)\n",
    "    )  \n",
    "    layer1_output = layer1.output.flatten(2)\n",
    "\n",
    "    layer2 = HiddenLayer(\n",
    "        input=layer1_output,\n",
    "        n_in=nkerns[1]*4*4,\n",
    "        n_out=50,\n",
    "    )\n",
    "\n",
    "    layer3 = MultiLogisticRegression(input=layer2.output, n_in=50, n_out=10)\n",
    "\n",
    "    cost = layer3.nll(y)\n",
    "\n",
    "\n",
    "    model_predict = theano.function(\n",
    "        [x],\n",
    "        layer3.predict\n",
    "    )\n",
    "\n",
    "\n",
    "    params = layer3.params + layer2.params + layer1.params + layer0.params\n",
    "    grads = T.grad(cost, params)\n",
    "    updates = [\n",
    "        (param_i, param_i - learning_rate * grad_i)\n",
    "        for param_i, grad_i in zip(params, grads)\n",
    "    ] \n",
    "\n",
    "    train_model = theano.function(\n",
    "        [x, y],\n",
    "        cost,\n",
    "        updates=updates\n",
    "    )\n",
    "    \n",
    "    dataset = 'digits.pkl.gz' \n",
    "    f = gzip.open(dataset, 'rb')\n",
    "    train_set, valid_set, test_set = cPickle.load(f)\n",
    "    f.close()\n",
    "    train_set_x, train_set_y = train_set\n",
    "    test_set_x, test_set_y = test_set\n",
    "    train_set_y = train_set_y.astype(numpy.int32)\n",
    "\n",
    "    ix = []\n",
    "    for i in range(10):\n",
    "        ix.append(numpy.nonzero(train_set_y == i)[0][:500])\n",
    "    ix = numpy.concatenate(ix)\n",
    "    train_set_x = train_set_x[ix]\n",
    "    train_set_y = train_set_y[ix]\n",
    "    ix = numpy.random.permutation(train_set_x.shape[0])\n",
    "    train_set_x = train_set_x[ix]\n",
    "    train_set_y = train_set_y[ix]\n",
    "\n",
    "    n_batches = train_set_x.shape[0]\n",
    "    n_batches /= batch_size\n",
    "    \n",
    "    n_epochs = num_epochs\n",
    "    c = numpy.zeros((n_epochs,))\n",
    "    for i in range(n_epochs): \n",
    "        err = 0\n",
    "        for b in range(n_batches):\n",
    "            train_model(train_set_x[b*batch_size:(b+1)*batch_size], train_set_y[b*batch_size:(b+1)*batch_size])\n",
    "            err += train_model(train_set_x[b*batch_size:(b+1)*batch_size], train_set_y[b*batch_size:(b+1)*batch_size])\n",
    "        print 'iteration:', i, ', nll =', err\n",
    "        c[i] = err\n",
    "  \n",
    "    n_testbatches = test_set_x.shape[0] / batch_size\n",
    "    err = 0\n",
    "    for b in range(n_testbatches):\n",
    "        yp = model_predict(test_set_x[b*batch_size:(b+1)*batch_size])\n",
    "        yy = test_set_y[b*batch_size:(b+1)*batch_size]\n",
    "        err += len(np.nonzero(yp - yy)[0])\n",
    "    err=1.0*err/len(test_set_y)\n",
    "    \n",
    "    \n",
    "    return layer0.W,err ##just make the Making ConvNN function returns the filters and error rate"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Task I"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "global name 'conv' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-12-bc98faf0bb72>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;31m#########Task I################\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0mres\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mConvNN_Origin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m400\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;31m## just make the Making ConvNN function return the filters\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      3\u001b[0m \u001b[0mfilters\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mres\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0mplt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfigure\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mi\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m10\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-11-a1ab80886f10>\u001b[0m in \u001b[0;36mConvNN_Origin\u001b[0;34m(num_epochs)\u001b[0m\n\u001b[1;32m     14\u001b[0m         \u001b[0mimage_shape\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbatch_size\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m28\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m28\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     15\u001b[0m         \u001b[0mfilter_shape\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnkerns\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m5\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m5\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 16\u001b[0;31m         \u001b[0mpoolsize\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m2\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     17\u001b[0m     )\n\u001b[1;32m     18\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-10-a330b47d55d5>\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, input, filter_shape, image_shape, poolsize)\u001b[0m\n\u001b[1;32m     15\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mparams\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mW\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mb\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     16\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 17\u001b[0;31m         conv_out = conv.conv2d(\n\u001b[0m\u001b[1;32m     18\u001b[0m             \u001b[0minput\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     19\u001b[0m             \u001b[0mfilters\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mW\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: global name 'conv' is not defined"
     ]
    }
   ],
   "source": [
    "#########Task I################\n",
    "res=ConvNN_Origin(400) ## just make the Making ConvNN function return the filters\n",
    "filters=res[0]\n",
    "plt.figure(1)\n",
    "for i in range(0,10):\n",
    "    plt.subplot(2,5,i), imshow(filters.get_value()[i][0],cmap=cm.gray)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Task 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "#########Task II################\n",
    "def ConvNN_3_Layers(num_epochs):\n",
    "    batch_size = 250\n",
    "    learning_rate=0.1\n",
    "    nkerns=[10,10]\n",
    "\n",
    "\n",
    "    x = T.matrix('x')\n",
    "    y = T.ivector('y')\n",
    "\n",
    "    layer0_input = x.reshape((batch_size, 1, 28, 28))\n",
    "    layer0 = ConvPoolLayer(\n",
    "        input=layer0_input,\n",
    "        image_shape=(batch_size, 1, 28, 28),\n",
    "        filter_shape=(nkerns[0], 1, 5, 5),\n",
    "        poolsize=(2, 2)\n",
    "    )\n",
    "\n",
    "    layer0_output = layer0.output.flatten(2)\n",
    "    \n",
    "    layer2 = HiddenLayer(\n",
    "        input=layer0_output,\n",
    "        n_in=nkerns[1]*12*12,\n",
    "        n_out=50,\n",
    "    )\n",
    "\n",
    "    layer3 = MultiLogisticRegression(input=layer2.output, n_in=50, n_out=10)\n",
    "\n",
    "    cost = layer3.nll(y)\n",
    "\n",
    "\n",
    "    model_predict = theano.function(\n",
    "        [x],\n",
    "        layer3.predict\n",
    "    )\n",
    "\n",
    "    params = layer3.params + layer2.params + layer0.params\n",
    "    grads = T.grad(cost, params)\n",
    "    updates = [\n",
    "        (param_i, param_i - learning_rate * grad_i)\n",
    "        for param_i, grad_i in zip(params, grads)\n",
    "    ] \n",
    "\n",
    "    train_model = theano.function(\n",
    "        [x, y],\n",
    "        cost,\n",
    "        updates=updates\n",
    "    )\n",
    "    \n",
    "    dataset = 'digits.pkl.gz' \n",
    "    f = gzip.open(dataset, 'rb')\n",
    "    train_set, valid_set, test_set = cPickle.load(f)\n",
    "    f.close()\n",
    "    train_set_x, train_set_y = train_set\n",
    "    test_set_x, test_set_y = test_set\n",
    "    train_set_y = train_set_y.astype(numpy.int32)\n",
    "\n",
    "    ix = []\n",
    "    for i in range(10):\n",
    "        ix.append(numpy.nonzero(train_set_y == i)[0][:500])\n",
    "    ix = numpy.concatenate(ix)\n",
    "    train_set_x = train_set_x[ix]\n",
    "    train_set_y = train_set_y[ix]\n",
    "    ix = numpy.random.permutation(train_set_x.shape[0])\n",
    "    train_set_x = train_set_x[ix]\n",
    "    train_set_y = train_set_y[ix]\n",
    "\n",
    "    n_batches = train_set_x.shape[0]\n",
    "    n_batches /= batch_size\n",
    "    \n",
    "    n_epochs = num_epochs\n",
    "    c = numpy.zeros((n_epochs,))\n",
    "    for i in range(n_epochs): \n",
    "        err = 0\n",
    "        for b in range(n_batches):\n",
    "            train_model(train_set_x[b*batch_size:(b+1)*batch_size], train_set_y[b*batch_size:(b+1)*batch_size])\n",
    "            err += train_model(train_set_x[b*batch_size:(b+1)*batch_size], train_set_y[b*batch_size:(b+1)*batch_size])\n",
    "        print 'iteration:', i, ', nll =', err\n",
    "        c[i] = err\n",
    "  \n",
    "    n_testbatches = test_set_x.shape[0] / batch_size\n",
    "    err = 0\n",
    "    for b in range(n_testbatches):\n",
    "        yp = model_predict(test_set_x[b*batch_size:(b+1)*batch_size])\n",
    "        yy = test_set_y[b*batch_size:(b+1)*batch_size]\n",
    "        err += len(np.nonzero(yp - yy)[0])\n",
    "    return 1.0*err/len(test_set_y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def ConvNN_2_Layers(num_epochs):\n",
    "    batch_size = 250\n",
    "    learning_rate=0.1\n",
    "    nkerns=[10,10]\n",
    "\n",
    "\n",
    "    x = T.matrix('x')\n",
    "    y = T.ivector('y')\n",
    "\n",
    " \n",
    "    layer2_input = x.reshape((batch_size, 1, 28, 28)).flatten(2)\n",
    "    \n",
    "    layer2 = HiddenLayer(\n",
    "        input=layer2_input,\n",
    "        n_in=28*28,\n",
    "        n_out=50,\n",
    "    )\n",
    "\n",
    "    layer3 = MultiLogisticRegression(input=layer2.output, n_in=50, n_out=10)\n",
    "\n",
    "    cost = layer3.nll(y)\n",
    "\n",
    "\n",
    "    model_predict = theano.function(\n",
    "        [x],\n",
    "        layer3.predict\n",
    "    )\n",
    "\n",
    "    params = layer3.params + layer2.params \n",
    "    grads = T.grad(cost, params)\n",
    "    updates = [\n",
    "        (param_i, param_i - learning_rate * grad_i)\n",
    "        for param_i, grad_i in zip(params, grads)\n",
    "    ] \n",
    "\n",
    "    train_model = theano.function(\n",
    "        [x, y],\n",
    "        cost,\n",
    "        updates=updates\n",
    "    )\n",
    "    \n",
    "    dataset = 'digits.pkl.gz' \n",
    "    f = gzip.open(dataset, 'rb')\n",
    "    train_set, valid_set, test_set = cPickle.load(f)\n",
    "    f.close()\n",
    "    train_set_x, train_set_y = train_set\n",
    "    test_set_x, test_set_y = test_set\n",
    "    train_set_y = train_set_y.astype(numpy.int32)\n",
    "\n",
    "    ix = []\n",
    "    for i in range(10):\n",
    "        ix.append(numpy.nonzero(train_set_y == i)[0][:500])\n",
    "    ix = numpy.concatenate(ix)\n",
    "    train_set_x = train_set_x[ix]\n",
    "    train_set_y = train_set_y[ix]\n",
    "    ix = numpy.random.permutation(train_set_x.shape[0])\n",
    "    train_set_x = train_set_x[ix]\n",
    "    train_set_y = train_set_y[ix]\n",
    "\n",
    "    n_batches = train_set_x.shape[0]\n",
    "    n_batches /= batch_size\n",
    "    \n",
    "    n_epochs = num_epochs\n",
    "    c = numpy.zeros((n_epochs,))\n",
    "    for i in range(n_epochs): \n",
    "        err = 0\n",
    "        for b in range(n_batches):\n",
    "            train_model(train_set_x[b*batch_size:(b+1)*batch_size], train_set_y[b*batch_size:(b+1)*batch_size])\n",
    "            err += train_model(train_set_x[b*batch_size:(b+1)*batch_size], train_set_y[b*batch_size:(b+1)*batch_size])\n",
    "        print 'iteration:', i, ', nll =', err\n",
    "        c[i] = err\n",
    "  \n",
    "    n_testbatches = test_set_x.shape[0] / batch_size\n",
    "    err = 0\n",
    "    for b in range(n_testbatches):\n",
    "        yp = model_predict(test_set_x[b*batch_size:(b+1)*batch_size])\n",
    "        yy = test_set_y[b*batch_size:(b+1)*batch_size]\n",
    "        err += len(np.nonzero(yp - yy)[0])\n",
    "    return 1.0*err/len(test_set_y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "global name 'conv' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-15-3b90144fd525>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;31m## Original CNN\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0mtime_begin\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtime\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtime\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 3\u001b[0;31m \u001b[0mres\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mConvNN_Origin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m400\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      4\u001b[0m \u001b[0mtime_end\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtime\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtime\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0merr\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mres\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-11-a1ab80886f10>\u001b[0m in \u001b[0;36mConvNN_Origin\u001b[0;34m(num_epochs)\u001b[0m\n\u001b[1;32m     14\u001b[0m         \u001b[0mimage_shape\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbatch_size\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m28\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m28\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     15\u001b[0m         \u001b[0mfilter_shape\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnkerns\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m5\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m5\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 16\u001b[0;31m         \u001b[0mpoolsize\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m2\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     17\u001b[0m     )\n\u001b[1;32m     18\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-10-a330b47d55d5>\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, input, filter_shape, image_shape, poolsize)\u001b[0m\n\u001b[1;32m     15\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mparams\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mW\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mb\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     16\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 17\u001b[0;31m         conv_out = conv.conv2d(\n\u001b[0m\u001b[1;32m     18\u001b[0m             \u001b[0minput\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     19\u001b[0m             \u001b[0mfilters\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mW\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: global name 'conv' is not defined"
     ]
    }
   ],
   "source": [
    "## Original CNN\n",
    "time_begin=time.time()\n",
    "res=ConvNN_Origin(400) \n",
    "time_end=time.time()\n",
    "err=res[1]\n",
    "nminutes=(time_end-time_begin)/60\n",
    "print \"The error rate of the original CNN is \"+str(err)\n",
    "print \"The time used for the original CNN is \"+str(nminutes)+\" minutes\"\n",
    "\n",
    "##1 con&pool layer, 1 hidden layer and 1 output layer\n",
    "time_begin=time.time()\n",
    "err=ConvNN_3_Layers(400)\n",
    "time_end=time.time()\n",
    "nminutes=(time_end-time_begin)/60\n",
    "print \"The error rate of 3 layers: \"+str(err)\n",
    "print \"The time used for 3 layers: \"+str(nminutes)+\" minutes\"\n",
    "\n",
    "##1 hidden layer and 1 output layer \n",
    "time_begin=time.time()\n",
    "err=ConvNN_2_Layers(400)\n",
    "time_end=time.time()\n",
    "nminutes=(time_end-time_begin)/60\n",
    "print \"The error rate of 2 layers: \"+str(err)\n",
    "print \"The time used for 2 layers: \"+str(nminutes)+\" minutes\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The error rates of these three cases are comparable. However, it is noticable that the performance is much better with the conv&pool layers."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Task 3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "def ConvNN_Diff_Filters(num_epochs,filter_num0,filter_num1):\n",
    "    batch_size = 250\n",
    "    learning_rate=0.1\n",
    "    nkerns=[filter_num0, filter_num1]\n",
    "\n",
    "\n",
    "    x = T.matrix('x')\n",
    "    y = T.ivector('y')\n",
    "\n",
    "\n",
    "    layer0_input = x.reshape((batch_size, 1, 28, 28))\n",
    "    layer0 = ConvPoolLayer(\n",
    "        input=layer0_input,\n",
    "        image_shape=(batch_size, 1, 28, 28),\n",
    "        filter_shape=(nkerns[0], 1, 5, 5),\n",
    "        poolsize=(2, 2)\n",
    "    )\n",
    "\n",
    "    layer1 = ConvPoolLayer(\n",
    "        input=layer0.output,\n",
    "        image_shape=(batch_size, nkerns[0], 12, 12),\n",
    "        filter_shape=(nkerns[1], nkerns[0], 5, 5),\n",
    "        poolsize=(2, 2)\n",
    "    )  \n",
    "    layer1_output = layer1.output.flatten(2)\n",
    "\n",
    "    layer2 = HiddenLayer(\n",
    "        input=layer1_output,\n",
    "        n_in=nkerns[1]*4*4,\n",
    "        n_out=50,\n",
    "    )\n",
    "\n",
    "    layer3 = MultiLogisticRegression(input=layer2.output, n_in=50, n_out=10)\n",
    "\n",
    "    cost = layer3.nll(y)\n",
    "\n",
    "\n",
    "    model_predict = theano.function(\n",
    "        [x],\n",
    "        layer3.predict\n",
    "    )\n",
    "\n",
    "\n",
    "    params = layer3.params + layer2.params + layer1.params + layer0.params\n",
    "    grads = T.grad(cost, params)\n",
    "    updates = [\n",
    "        (param_i, param_i - learning_rate * grad_i)\n",
    "        for param_i, grad_i in zip(params, grads)\n",
    "    ] \n",
    "\n",
    "    train_model = theano.function(\n",
    "        [x, y],\n",
    "        cost,\n",
    "        updates=updates\n",
    "    )\n",
    "    \n",
    "    dataset = 'digits.pkl.gz' \n",
    "    f = gzip.open(dataset, 'rb')\n",
    "    train_set, valid_set, test_set = cPickle.load(f)\n",
    "    f.close()\n",
    "    train_set_x, train_set_y = train_set\n",
    "    test_set_x, test_set_y = test_set\n",
    "    train_set_y = train_set_y.astype(numpy.int32)\n",
    "\n",
    "    ix = []\n",
    "    for i in range(10):\n",
    "        ix.append(numpy.nonzero(train_set_y == i)[0][:500])\n",
    "    ix = numpy.concatenate(ix)\n",
    "    train_set_x = train_set_x[ix]\n",
    "    train_set_y = train_set_y[ix]\n",
    "    ix = numpy.random.permutation(train_set_x.shape[0])\n",
    "    train_set_x = train_set_x[ix]\n",
    "    train_set_y = train_set_y[ix]\n",
    "\n",
    "    n_batches = train_set_x.shape[0]\n",
    "    n_batches /= batch_size\n",
    "    \n",
    "    n_epochs = num_epochs\n",
    "    c = numpy.zeros((n_epochs,))\n",
    "    for i in range(n_epochs): \n",
    "        err = 0\n",
    "        for b in range(n_batches):\n",
    "            train_model(train_set_x[b*batch_size:(b+1)*batch_size], train_set_y[b*batch_size:(b+1)*batch_size])\n",
    "            err += train_model(train_set_x[b*batch_size:(b+1)*batch_size], train_set_y[b*batch_size:(b+1)*batch_size])\n",
    "        print 'iteration:', i, ', nll =', err\n",
    "        c[i] = err\n",
    "  \n",
    "    n_testbatches = test_set_x.shape[0] / batch_size\n",
    "    err = 0\n",
    "    for b in range(n_testbatches):\n",
    "        yp = model_predict(test_set_x[b*batch_size:(b+1)*batch_size])\n",
    "        yy = test_set_y[b*batch_size:(b+1)*batch_size]\n",
    "        err += len(np.nonzero(yp - yy)[0])\n",
    "    return 1.0*err/len(test_set_y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "global name 'conv' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-17-be10b8885be0>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;31m#10 filters for layer1 and 20 filters for layer2\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0mtime_begin\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtime\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtime\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 3\u001b[0;31m \u001b[0merr\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mConvNN_Diff_Filters\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m400\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m10\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m20\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      4\u001b[0m \u001b[0mtime_end\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtime\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtime\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0mnminutes\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtime_end\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0mtime_begin\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m/\u001b[0m\u001b[0;36m60\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-16-3abe818b6bca>\u001b[0m in \u001b[0;36mConvNN_Diff_Filters\u001b[0;34m(num_epochs, filter_num0, filter_num1)\u001b[0m\n\u001b[1;32m     14\u001b[0m         \u001b[0mimage_shape\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbatch_size\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m28\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m28\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     15\u001b[0m         \u001b[0mfilter_shape\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnkerns\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m5\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m5\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 16\u001b[0;31m         \u001b[0mpoolsize\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m2\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     17\u001b[0m     )\n\u001b[1;32m     18\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-10-a330b47d55d5>\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, input, filter_shape, image_shape, poolsize)\u001b[0m\n\u001b[1;32m     15\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mparams\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mW\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mb\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     16\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 17\u001b[0;31m         conv_out = conv.conv2d(\n\u001b[0m\u001b[1;32m     18\u001b[0m             \u001b[0minput\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     19\u001b[0m             \u001b[0mfilters\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mW\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: global name 'conv' is not defined"
     ]
    }
   ],
   "source": [
    "#10 filters for layer1 and 20 filters for layer2\n",
    "time_begin=time.time()\n",
    "err=ConvNN_Diff_Filters(400,10,20)\n",
    "time_end=time.time()\n",
    "nminutes=(time_end-time_begin)/60\n",
    "print \"The error rate of 10 filters for layer1 and 20 filters for layer2: \"+str(err)\n",
    "print \"The time used for 10 filters for layer1 and 20 filters for layer2: \"+str(nminutes)+\" minutes\"\n",
    "\n",
    "#20 filters for layer1 and 10 filters for layer2\n",
    "time_begin=time.time()\n",
    "ConvNN_Diff_Filters(400,20,10)\n",
    "time_end=time.time()\n",
    "nminutes=(time_end-time_begin)/60\n",
    "print \"The error rate of 20 filters for layer1 and 10 filters for layer2: \"+str(err)\n",
    "print \"The time used for 20 filters for layer1 and 10 filters for layer2: \"+str(nminutes)+\" minutes\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "With the iteration number 1200, the error rates of these two cases are the same as that of the original. However, more filters will leader to significantly worse performance. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
