{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## HW2\n",
    "#### Due Mar. 18th by end of day. Name your notebook as firstname.lastname.HW2.ipynb and email it to zhang@csc.lsu.edu\n",
    "\n",
    "Your tasks in this homework are to experiment with CNN.\n",
    "A simple CNN is given below. \n",
    "\n",
    "### Task 1\n",
    "Train the CNN (as much as you can) to reach convergence. Investigate what patterns the first layer (layer 0) filters pick up by plotting the filters as small 2d images. To plot a 2d array x as image, use \"imshow(x, cmap=cm.gray)\". You should plot the 10 filters together using subplot. \n",
    "\n",
    "### Task 2\n",
    "The given CNN has 2 conv&pool layers, 1 hidden layer and 1 output layer. \n",
    "Modify the CNN to have:\n",
    "  - 1 conv&pool layer, 1 hidden layer and 1 output layer;\n",
    "  - 1 hidden layer and 1 output layer; \n",
    "  \n",
    "while keeping the other parameters the same. Compare the error rates on the test data for the original CNN and the two modifications and determine whether the conv&pool layers play a significant role for performance.\n",
    "\n",
    "### Task 3\n",
    "Change the number of filters for the two conv&pool layers:\n",
    "  - try 10 filters for layer 1 and 20 for layer 2;\n",
    "  - try 20 filters for layer 1 and 10 for layer 2.\n",
    "\n",
    "Compare error rate of the two cases and that of the original. Comment on how number of filters can impact performance. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Populating the interactive namespace from numpy and matplotlib\n"
     ]
    }
   ],
   "source": [
    "%pylab inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import cPickle, gzip\n",
    "\n",
    "import numpy\n",
    "import time\n",
    "\n",
    "import theano\n",
    "import theano.tensor as T\n",
    "from theano.tensor.signal import downsample\n",
    "from theano.tensor.nnet import conv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class HiddenLayer(object):\n",
    "\n",
    "    def __init__(self, input, n_in, n_out):\n",
    "        self.input = input\n",
    "\n",
    "        W_values = 4*numpy.random.uniform(\n",
    "                low=-numpy.sqrt(6. / (n_in + n_out)),\n",
    "                high=numpy.sqrt(6. / (n_in + n_out)),\n",
    "                size=(n_in, n_out)\n",
    "        )\n",
    "        self.W = theano.shared(value=W_values, name='W', borrow=True)\n",
    "        self.b = theano.shared(value=numpy.zeros((n_out,)), name='b', borrow=True)\n",
    "        self.params = [self.W, self.b]\n",
    "\n",
    "        net = T.dot(self.input, self.W) + self.b\n",
    "        self.output = T.nnet.sigmoid(net)\n",
    "\n",
    "        \n",
    "class MultiLogisticRegression(object):\n",
    "\n",
    "    def __init__(self, input, n_in, n_out):\n",
    "\n",
    "        self.input = input\n",
    "\n",
    "        self.W = theano.shared(\n",
    "            value=numpy.zeros((n_in, n_out)),\n",
    "            name='W',\n",
    "            borrow=True\n",
    "        )\n",
    "        self.b = theano.shared(\n",
    "            value = numpy.zeros((n_out,)),\n",
    "            name='b',\n",
    "            borrow=True\n",
    "        )\n",
    "        self.params = [self.W, self.b]\n",
    "\n",
    "        self.prob = T.nnet.softmax(T.dot(self.input, self.W) + self.b)\n",
    "        self.predict = T.argmax(self.prob, axis=1)\n",
    "\n",
    "    def nll(self, y):\n",
    "        return  -T.mean(T.log(self.prob)[T.arange(y.shape[0]), y])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class ConvPoolLayer(object):\n",
    "\n",
    "    def __init__(self, input, filter_shape, image_shape, poolsize):\n",
    "\n",
    "        self.input = input\n",
    "\n",
    "        n_in = filter_shape[1]*filter_shape[2]*filter_shape[3]\n",
    "        n_out = (filter_shape[0]*filter_shape[2]*filter_shape[3])/(poolsize[0]*poolsize[1])\n",
    "        W_bound = numpy.sqrt(6./(n_in + n_out))\n",
    "        self.W = theano.shared(\n",
    "            numpy.random.uniform(low=-W_bound, high=W_bound, size=filter_shape),\n",
    "            borrow=True\n",
    "        )\n",
    "        self.b = theano.shared(value=numpy.zeros((filter_shape[0],)), borrow=True)\n",
    "        self.params = [self.W, self.b]\n",
    "        \n",
    "        conv_out = conv.conv2d(\n",
    "            input=self.input,\n",
    "            filters=self.W,\n",
    "            filter_shape=filter_shape,\n",
    "            image_shape=image_shape\n",
    "        )\n",
    "\n",
    "        pooled_out = downsample.max_pool_2d(\n",
    "            input=conv_out,\n",
    "            ds=poolsize,\n",
    "            ignore_border=True\n",
    "        )\n",
    "\n",
    "        self.output = T.tanh(pooled_out + self.b.dimshuffle('x', 0, 'x', 'x'))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def ConvNN_Origin(num_epochs):\n",
    "    batch_size = 250\n",
    "    learning_rate=0.1\n",
    "    nkerns=[10, 10]\n",
    "\n",
    "\n",
    "    x = T.matrix('x')\n",
    "    y = T.ivector('y')\n",
    "\n",
    "\n",
    "    layer0_input = x.reshape((batch_size, 1, 28, 28))\n",
    "    layer0 = ConvPoolLayer(\n",
    "        input=layer0_input,\n",
    "        image_shape=(batch_size, 1, 28, 28),\n",
    "        filter_shape=(nkerns[0], 1, 5, 5),\n",
    "        poolsize=(2, 2)\n",
    "    )\n",
    "\n",
    "    layer1 = ConvPoolLayer(\n",
    "        input=layer0.output,\n",
    "        image_shape=(batch_size, nkerns[0], 12, 12),\n",
    "        filter_shape=(nkerns[1], nkerns[0], 5, 5),\n",
    "        poolsize=(2, 2)\n",
    "    )  \n",
    "    layer1_output = layer1.output.flatten(2)\n",
    "\n",
    "    layer2 = HiddenLayer(\n",
    "        input=layer1_output,\n",
    "        n_in=nkerns[1]*4*4,\n",
    "        n_out=50,\n",
    "    )\n",
    "\n",
    "    layer3 = MultiLogisticRegression(input=layer2.output, n_in=50, n_out=10)\n",
    "\n",
    "    cost = layer3.nll(y)\n",
    "\n",
    "\n",
    "    model_predict = theano.function(\n",
    "        [x],\n",
    "        layer3.predict\n",
    "    )\n",
    "\n",
    "\n",
    "    params = layer3.params + layer2.params + layer1.params + layer0.params\n",
    "    grads = T.grad(cost, params)\n",
    "    updates = [\n",
    "        (param_i, param_i - learning_rate * grad_i)\n",
    "        for param_i, grad_i in zip(params, grads)\n",
    "    ] \n",
    "\n",
    "    train_model = theano.function(\n",
    "        [x, y],\n",
    "        cost,\n",
    "        updates=updates\n",
    "    )\n",
    "    \n",
    "    dataset = 'digits.pkl.gz' \n",
    "    f = gzip.open(dataset, 'rb')\n",
    "    train_set, valid_set, test_set = cPickle.load(f)\n",
    "    f.close()\n",
    "    train_set_x, train_set_y = train_set\n",
    "    test_set_x, test_set_y = test_set\n",
    "    train_set_y = train_set_y.astype(numpy.int32)\n",
    "\n",
    "    ix = []\n",
    "    for i in range(10):\n",
    "        ix.append(numpy.nonzero(train_set_y == i)[0][:500])\n",
    "    ix = numpy.concatenate(ix)\n",
    "    train_set_x = train_set_x[ix]\n",
    "    train_set_y = train_set_y[ix]\n",
    "    ix = numpy.random.permutation(train_set_x.shape[0])\n",
    "    train_set_x = train_set_x[ix]\n",
    "    train_set_y = train_set_y[ix]\n",
    "\n",
    "    n_batches = train_set_x.shape[0]\n",
    "    n_batches /= batch_size\n",
    "    \n",
    "    n_epochs = num_epochs\n",
    "    c = numpy.zeros((n_epochs,))\n",
    "    for i in range(n_epochs): \n",
    "        err = 0\n",
    "        for b in range(n_batches):\n",
    "            train_model(train_set_x[b*batch_size:(b+1)*batch_size], train_set_y[b*batch_size:(b+1)*batch_size])\n",
    "            err += train_model(train_set_x[b*batch_size:(b+1)*batch_size], train_set_y[b*batch_size:(b+1)*batch_size])\n",
    "        print 'iteration:', i, ', nll =', err\n",
    "        c[i] = err\n",
    "  \n",
    "    n_testbatches = test_set_x.shape[0] / batch_size\n",
    "    err = 0\n",
    "    for b in range(n_testbatches):\n",
    "        yp = model_predict(test_set_x[b*batch_size:(b+1)*batch_size])\n",
    "        yy = test_set_y[b*batch_size:(b+1)*batch_size]\n",
    "        err += len(np.nonzero(yp - yy)[0])\n",
    "    err=1.0*err/len(test_set_y)\n",
    "    \n",
    "    \n",
    "    return layer0.W,err ##just make the Making ConvNN function returns the filters and error rate"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Task I"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 200,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "iteration: 0 , nll = 43.8158309645\n",
      "iteration: 1 , nll = 35.3132240402\n",
      "iteration: 2 , nll = 26.5545914706\n",
      "iteration: 3 , nll = 20.8633456036\n",
      "iteration: 4 , nll = 17.2401447693\n",
      "iteration: 5 , nll = 14.7505480526\n",
      "iteration: 6 , nll = 12.9237436843\n",
      "iteration: 7 , nll = 11.510712986\n",
      "iteration: 8 , nll = 10.3718668414\n",
      "iteration: 9 , nll = 9.42745676098\n",
      "iteration: 10 , nll = 8.6293803369\n",
      "iteration: 11 , nll = 7.94313831772\n",
      "iteration: 12 , nll = 7.34618714592\n",
      "iteration: 13 , nll = 6.82382539997\n",
      "iteration: 14 , nll = 6.36497394108\n",
      "iteration: 15 , nll = 5.95881000953\n",
      "iteration: 16 , nll = 5.59920048548\n",
      "iteration: 17 , nll = 5.27820045086\n",
      "iteration: 18 , nll = 4.98944298788\n",
      "iteration: 19 , nll = 4.72949064045\n",
      "iteration: 20 , nll = 4.49464254122\n",
      "iteration: 21 , nll = 4.2815342802\n",
      "iteration: 22 , nll = 4.08758593846\n",
      "iteration: 23 , nll = 3.91011522713\n",
      "iteration: 24 , nll = 3.74694827297\n",
      "iteration: 25 , nll = 3.59653800931\n",
      "iteration: 26 , nll = 3.45821090738\n",
      "iteration: 27 , nll = 3.33029557746\n",
      "iteration: 28 , nll = 3.21119515765\n",
      "iteration: 29 , nll = 3.09986081291\n",
      "iteration: 30 , nll = 2.99589180838\n",
      "iteration: 31 , nll = 2.89802901543\n",
      "iteration: 32 , nll = 2.80607533407\n",
      "iteration: 33 , nll = 2.71966563956\n",
      "iteration: 34 , nll = 2.63850323819\n",
      "iteration: 35 , nll = 2.56196781058\n",
      "iteration: 36 , nll = 2.4890807023\n",
      "iteration: 37 , nll = 2.42032051281\n",
      "iteration: 38 , nll = 2.35521197084\n",
      "iteration: 39 , nll = 2.29322937692\n",
      "iteration: 40 , nll = 2.23398560719\n",
      "iteration: 41 , nll = 2.17687437347\n",
      "iteration: 42 , nll = 2.12229681346\n",
      "iteration: 43 , nll = 2.07027363357\n",
      "iteration: 44 , nll = 2.02055185169\n",
      "iteration: 45 , nll = 1.97260628165\n",
      "iteration: 46 , nll = 1.92625731741\n",
      "iteration: 47 , nll = 1.88174466004\n",
      "iteration: 48 , nll = 1.83923013984\n",
      "iteration: 49 , nll = 1.79815827815\n",
      "iteration: 50 , nll = 1.75875235368\n",
      "iteration: 51 , nll = 1.72087854325\n",
      "iteration: 52 , nll = 1.68441816293\n",
      "iteration: 53 , nll = 1.64894044755\n",
      "iteration: 54 , nll = 1.61452659699\n",
      "iteration: 55 , nll = 1.58132888534\n",
      "iteration: 56 , nll = 1.54917238637\n",
      "iteration: 57 , nll = 1.51804540481\n",
      "iteration: 58 , nll = 1.48773153582\n",
      "iteration: 59 , nll = 1.45865972537\n",
      "iteration: 60 , nll = 1.43025128625\n",
      "iteration: 61 , nll = 1.40282156513\n",
      "iteration: 62 , nll = 1.37626251581\n",
      "iteration: 63 , nll = 1.35025069374\n",
      "iteration: 64 , nll = 1.32496643517\n",
      "iteration: 65 , nll = 1.30023597151\n",
      "iteration: 66 , nll = 1.27613386875\n",
      "iteration: 67 , nll = 1.25280607064\n",
      "iteration: 68 , nll = 1.22983084303\n",
      "iteration: 69 , nll = 1.20754448231\n",
      "iteration: 70 , nll = 1.18579987499\n",
      "iteration: 71 , nll = 1.1648886577\n",
      "iteration: 72 , nll = 1.14435758464\n",
      "iteration: 73 , nll = 1.1243351596\n",
      "iteration: 74 , nll = 1.10479118651\n",
      "iteration: 75 , nll = 1.08568683919\n",
      "iteration: 76 , nll = 1.06712782808\n",
      "iteration: 77 , nll = 1.04900694332\n",
      "iteration: 78 , nll = 1.03121274393\n",
      "iteration: 79 , nll = 1.0139025093\n",
      "iteration: 80 , nll = 0.997016869531\n",
      "iteration: 81 , nll = 0.980552843609\n",
      "iteration: 82 , nll = 0.964574744836\n",
      "iteration: 83 , nll = 0.948956226467\n",
      "iteration: 84 , nll = 0.933712874727\n",
      "iteration: 85 , nll = 0.918690552632\n",
      "iteration: 86 , nll = 0.904224601593\n",
      "iteration: 87 , nll = 0.890001853418\n",
      "iteration: 88 , nll = 0.87618308633\n",
      "iteration: 89 , nll = 0.8626594539\n",
      "iteration: 90 , nll = 0.849347263567\n",
      "iteration: 91 , nll = 0.836543344495\n",
      "iteration: 92 , nll = 0.823764392557\n",
      "iteration: 93 , nll = 0.811354513819\n",
      "iteration: 94 , nll = 0.799289192244\n",
      "iteration: 95 , nll = 0.787361049709\n",
      "iteration: 96 , nll = 0.775705719318\n",
      "iteration: 97 , nll = 0.764314971627\n",
      "iteration: 98 , nll = 0.752994595127\n",
      "iteration: 99 , nll = 0.742043737978\n",
      "iteration: 100 , nll = 0.731194101926\n",
      "iteration: 101 , nll = 0.720650832441\n",
      "iteration: 102 , nll = 0.710309316472\n",
      "iteration: 103 , nll = 0.700238843519\n",
      "iteration: 104 , nll = 0.690322225971\n",
      "iteration: 105 , nll = 0.680655575894\n",
      "iteration: 106 , nll = 0.671045161565\n",
      "iteration: 107 , nll = 0.661810343084\n",
      "iteration: 108 , nll = 0.652643265506\n",
      "iteration: 109 , nll = 0.643681951603\n",
      "iteration: 110 , nll = 0.634843685605\n",
      "iteration: 111 , nll = 0.626268645548\n",
      "iteration: 112 , nll = 0.61787757484\n",
      "iteration: 113 , nll = 0.609576139075\n",
      "iteration: 114 , nll = 0.601474995555\n",
      "iteration: 115 , nll = 0.593609448443\n",
      "iteration: 116 , nll = 0.585786657772\n",
      "iteration: 117 , nll = 0.578248524059\n",
      "iteration: 118 , nll = 0.570781291049\n",
      "iteration: 119 , nll = 0.563525006033\n",
      "iteration: 120 , nll = 0.556382819436\n",
      "iteration: 121 , nll = 0.549333458536\n",
      "iteration: 122 , nll = 0.54248535706\n",
      "iteration: 123 , nll = 0.535636464745\n",
      "iteration: 124 , nll = 0.529051553411\n",
      "iteration: 125 , nll = 0.522473466105\n",
      "iteration: 126 , nll = 0.516179829349\n",
      "iteration: 127 , nll = 0.509900580876\n",
      "iteration: 128 , nll = 0.503792633707\n",
      "iteration: 129 , nll = 0.497730031267\n",
      "iteration: 130 , nll = 0.49187288399\n",
      "iteration: 131 , nll = 0.486121920273\n",
      "iteration: 132 , nll = 0.480494132312\n",
      "iteration: 133 , nll = 0.474900982194\n",
      "iteration: 134 , nll = 0.469490042035\n",
      "iteration: 135 , nll = 0.464145564044\n",
      "iteration: 136 , nll = 0.458919626014\n",
      "iteration: 137 , nll = 0.453782767923\n",
      "iteration: 138 , nll = 0.448735246946\n",
      "iteration: 139 , nll = 0.443770538463\n",
      "iteration: 140 , nll = 0.438941343643\n",
      "iteration: 141 , nll = 0.43417453153\n",
      "iteration: 142 , nll = 0.429493896077\n",
      "iteration: 143 , nll = 0.424825998612\n",
      "iteration: 144 , nll = 0.420306587802\n",
      "iteration: 145 , nll = 0.415837068973\n",
      "iteration: 146 , nll = 0.411432486326\n",
      "iteration: 147 , nll = 0.407146257963\n",
      "iteration: 148 , nll = 0.402914832008\n",
      "iteration: 149 , nll = 0.39875384033\n",
      "iteration: 150 , nll = 0.39465732121\n",
      "iteration: 151 , nll = 0.390683392478\n",
      "iteration: 152 , nll = 0.386773661404\n",
      "iteration: 153 , nll = 0.382864001705\n",
      "iteration: 154 , nll = 0.379068671609\n",
      "iteration: 155 , nll = 0.375350697345\n",
      "iteration: 156 , nll = 0.371698394121\n",
      "iteration: 157 , nll = 0.368115740568\n",
      "iteration: 158 , nll = 0.364596687079\n",
      "iteration: 159 , nll = 0.361089231536\n",
      "iteration: 160 , nll = 0.357681389756\n",
      "iteration: 161 , nll = 0.354310131801\n",
      "iteration: 162 , nll = 0.351004672311\n",
      "iteration: 163 , nll = 0.347744968757\n",
      "iteration: 164 , nll = 0.344553287258\n",
      "iteration: 165 , nll = 0.341380779465\n",
      "iteration: 166 , nll = 0.338302387045\n",
      "iteration: 167 , nll = 0.335199000771\n",
      "iteration: 168 , nll = 0.332168506491\n",
      "iteration: 169 , nll = 0.32922971121\n",
      "iteration: 170 , nll = 0.326296581785\n",
      "iteration: 171 , nll = 0.323426757349\n",
      "iteration: 172 , nll = 0.320604234821\n",
      "iteration: 173 , nll = 0.317813659531\n",
      "iteration: 174 , nll = 0.315052068905\n",
      "iteration: 175 , nll = 0.312356905644\n",
      "iteration: 176 , nll = 0.309719296731\n",
      "iteration: 177 , nll = 0.307084828826\n",
      "iteration: 178 , nll = 0.304494169366\n",
      "iteration: 179 , nll = 0.301945474709\n",
      "iteration: 180 , nll = 0.29942566026\n",
      "iteration: 181 , nll = 0.2969530457\n",
      "iteration: 182 , nll = 0.294508778776\n",
      "iteration: 183 , nll = 0.292107834652\n",
      "iteration: 184 , nll = 0.289735336922\n",
      "iteration: 185 , nll = 0.287414536016\n",
      "iteration: 186 , nll = 0.285117760616\n",
      "iteration: 187 , nll = 0.2828654359\n",
      "iteration: 188 , nll = 0.280605345721\n",
      "iteration: 189 , nll = 0.278410362745\n",
      "iteration: 190 , nll = 0.27622480863\n",
      "iteration: 191 , nll = 0.274064615524\n",
      "iteration: 192 , nll = 0.271955166884\n",
      "iteration: 193 , nll = 0.269858348508\n",
      "iteration: 194 , nll = 0.267811518892\n",
      "iteration: 195 , nll = 0.265775326602\n",
      "iteration: 196 , nll = 0.263751845825\n",
      "iteration: 197 , nll = 0.261778003172\n",
      "iteration: 198 , nll = 0.259830746083\n",
      "iteration: 199 , nll = 0.257875183331\n",
      "iteration: 200 , nll = 0.255978296723\n",
      "iteration: 201 , nll = 0.254089652652\n",
      "iteration: 202 , nll = 0.252207304284\n",
      "iteration: 203 , nll = 0.250373816555\n",
      "iteration: 204 , nll = 0.248580479112\n",
      "iteration: 205 , nll = 0.24676839302\n",
      "iteration: 206 , nll = 0.245015911617\n",
      "iteration: 207 , nll = 0.243253373015\n",
      "iteration: 208 , nll = 0.241537587748\n",
      "iteration: 209 , nll = 0.239825443534\n",
      "iteration: 210 , nll = 0.238140836732\n",
      "iteration: 211 , nll = 0.236479492971\n",
      "iteration: 212 , nll = 0.234820193843\n",
      "iteration: 213 , nll = 0.233220869419\n",
      "iteration: 214 , nll = 0.231604723257\n",
      "iteration: 215 , nll = 0.230012099664\n",
      "iteration: 216 , nll = 0.228431347957\n",
      "iteration: 217 , nll = 0.226895253873\n",
      "iteration: 218 , nll = 0.225350500705\n",
      "iteration: 219 , nll = 0.223853242938\n",
      "iteration: 220 , nll = 0.222321885212\n",
      "iteration: 221 , nll = 0.220855611866\n",
      "iteration: 222 , nll = 0.219378826168\n",
      "iteration: 223 , nll = 0.217944306251\n",
      "iteration: 224 , nll = 0.216515282265\n",
      "iteration: 225 , nll = 0.215088346916\n",
      "iteration: 226 , nll = 0.213709032494\n",
      "iteration: 227 , nll = 0.212297975725\n",
      "iteration: 228 , nll = 0.210941623736\n",
      "iteration: 229 , nll = 0.209594742022\n",
      "iteration: 230 , nll = 0.208242337752\n",
      "iteration: 231 , nll = 0.206921326804\n",
      "iteration: 232 , nll = 0.205586984462\n",
      "iteration: 233 , nll = 0.204309624291\n",
      "iteration: 234 , nll = 0.203032042531\n",
      "iteration: 235 , nll = 0.201733340956\n",
      "iteration: 236 , nll = 0.200482427518\n",
      "iteration: 237 , nll = 0.199250123987\n",
      "iteration: 238 , nll = 0.198012161467\n",
      "iteration: 239 , nll = 0.196813455337\n",
      "iteration: 240 , nll = 0.195599236635\n",
      "iteration: 241 , nll = 0.194413067776\n",
      "iteration: 242 , nll = 0.193241458874\n",
      "iteration: 243 , nll = 0.192095630605\n",
      "iteration: 244 , nll = 0.190921405854\n",
      "iteration: 245 , nll = 0.189788732208\n",
      "iteration: 246 , nll = 0.188674236876\n",
      "iteration: 247 , nll = 0.187552907467\n",
      "iteration: 248 , nll = 0.186453859967\n",
      "iteration: 249 , nll = 0.185346077395\n",
      "iteration: 250 , nll = 0.18426601098\n",
      "iteration: 251 , nll = 0.183205859656\n",
      "iteration: 252 , nll = 0.182140399772\n",
      "iteration: 253 , nll = 0.181075121124\n",
      "iteration: 254 , nll = 0.180063585645\n",
      "iteration: 255 , nll = 0.179017062197\n",
      "iteration: 256 , nll = 0.177996830327\n",
      "iteration: 257 , nll = 0.176993945534\n",
      "iteration: 258 , nll = 0.175996886097\n",
      "iteration: 259 , nll = 0.174995770569\n",
      "iteration: 260 , nll = 0.174037763648\n",
      "iteration: 261 , nll = 0.173062280112\n",
      "iteration: 262 , nll = 0.172099661646\n",
      "iteration: 263 , nll = 0.171164094035\n",
      "iteration: 264 , nll = 0.170220095987\n",
      "iteration: 265 , nll = 0.169288603461\n",
      "iteration: 266 , nll = 0.168372364296\n",
      "iteration: 267 , nll = 0.167456453578\n",
      "iteration: 268 , nll = 0.166563483191\n",
      "iteration: 269 , nll = 0.165666234651\n",
      "iteration: 270 , nll = 0.164775633444\n",
      "iteration: 271 , nll = 0.163898259015\n",
      "iteration: 272 , nll = 0.163034833568\n",
      "iteration: 273 , nll = 0.162171852014\n",
      "iteration: 274 , nll = 0.161318281676\n",
      "iteration: 275 , nll = 0.160476042982\n",
      "iteration: 276 , nll = 0.159627836304\n",
      "iteration: 277 , nll = 0.158807116905\n",
      "iteration: 278 , nll = 0.15799391007\n",
      "iteration: 279 , nll = 0.157170931566\n",
      "iteration: 280 , nll = 0.156360385118\n",
      "iteration: 281 , nll = 0.155563167125\n",
      "iteration: 282 , nll = 0.154769389575\n",
      "iteration: 283 , nll = 0.153973120826\n",
      "iteration: 284 , nll = 0.153208831978\n",
      "iteration: 285 , nll = 0.152433757374\n",
      "iteration: 286 , nll = 0.151665196577\n",
      "iteration: 287 , nll = 0.150918565327\n",
      "iteration: 288 , nll = 0.150160725472\n",
      "iteration: 289 , nll = 0.149417522874\n",
      "iteration: 290 , nll = 0.148683101771\n",
      "iteration: 291 , nll = 0.147962458885\n",
      "iteration: 292 , nll = 0.14723776414\n",
      "iteration: 293 , nll = 0.146523178419\n",
      "iteration: 294 , nll = 0.145805142788\n",
      "iteration: 295 , nll = 0.145111698398\n",
      "iteration: 296 , nll = 0.144407366454\n",
      "iteration: 297 , nll = 0.143714012465\n",
      "iteration: 298 , nll = 0.143029550731\n",
      "iteration: 299 , nll = 0.14234749226\n",
      "iteration: 300 , nll = 0.141679104487\n",
      "iteration: 301 , nll = 0.141000772516\n",
      "iteration: 302 , nll = 0.140345458392\n",
      "iteration: 303 , nll = 0.139691286727\n",
      "iteration: 304 , nll = 0.139036068467\n",
      "iteration: 305 , nll = 0.138401953053\n",
      "iteration: 306 , nll = 0.137743458168\n",
      "iteration: 307 , nll = 0.137118389511\n",
      "iteration: 308 , nll = 0.136488277199\n",
      "iteration: 309 , nll = 0.135864071019\n",
      "iteration: 310 , nll = 0.135233691001\n",
      "iteration: 311 , nll = 0.134633576126\n",
      "iteration: 312 , nll = 0.134029064188\n",
      "iteration: 313 , nll = 0.133419149184\n",
      "iteration: 314 , nll = 0.132823071873\n",
      "iteration: 315 , nll = 0.132223503824\n",
      "iteration: 316 , nll = 0.131631871959\n",
      "iteration: 317 , nll = 0.131051582671\n",
      "iteration: 318 , nll = 0.130471238574\n",
      "iteration: 319 , nll = 0.129894931636\n",
      "iteration: 320 , nll = 0.129319394123\n",
      "iteration: 321 , nll = 0.128767812277\n",
      "iteration: 322 , nll = 0.128202939981\n",
      "iteration: 323 , nll = 0.127648372896\n",
      "iteration: 324 , nll = 0.127087912673\n",
      "iteration: 325 , nll = 0.126544934626\n",
      "iteration: 326 , nll = 0.1259995759\n",
      "iteration: 327 , nll = 0.125462213142\n",
      "iteration: 328 , nll = 0.124926862054\n",
      "iteration: 329 , nll = 0.124395881003\n",
      "iteration: 330 , nll = 0.123873724014\n",
      "iteration: 331 , nll = 0.123340721139\n",
      "iteration: 332 , nll = 0.122825521361\n",
      "iteration: 333 , nll = 0.122308011671\n",
      "iteration: 334 , nll = 0.121805423543\n",
      "iteration: 335 , nll = 0.121294224004\n",
      "iteration: 336 , nll = 0.12078984761\n",
      "iteration: 337 , nll = 0.12029771739\n",
      "iteration: 338 , nll = 0.119793552908\n",
      "iteration: 339 , nll = 0.11930424375\n",
      "iteration: 340 , nll = 0.11881927809\n",
      "iteration: 341 , nll = 0.118334792461\n",
      "iteration: 342 , nll = 0.117848568605\n",
      "iteration: 343 , nll = 0.117370728219\n",
      "iteration: 344 , nll = 0.116903452418\n",
      "iteration: 345 , nll = 0.116428509235\n",
      "iteration: 346 , nll = 0.115958331515\n",
      "iteration: 347 , nll = 0.115499626133\n",
      "iteration: 348 , nll = 0.115044246091\n",
      "iteration: 349 , nll = 0.114577446031\n",
      "iteration: 350 , nll = 0.114124656187\n",
      "iteration: 351 , nll = 0.113678104278\n",
      "iteration: 352 , nll = 0.113235473839\n",
      "iteration: 353 , nll = 0.112781445392\n",
      "iteration: 354 , nll = 0.112343777105\n",
      "iteration: 355 , nll = 0.111908532006\n",
      "iteration: 356 , nll = 0.11147365147\n",
      "iteration: 357 , nll = 0.111043037651\n",
      "iteration: 358 , nll = 0.110614585059\n",
      "iteration: 359 , nll = 0.11018765409\n",
      "iteration: 360 , nll = 0.109759328816\n",
      "iteration: 361 , nll = 0.109344144419\n",
      "iteration: 362 , nll = 0.108928134962\n",
      "iteration: 363 , nll = 0.108514447395\n",
      "iteration: 364 , nll = 0.108105334625\n",
      "iteration: 365 , nll = 0.107700185814\n",
      "iteration: 366 , nll = 0.107292401587\n",
      "iteration: 367 , nll = 0.106892788632\n",
      "iteration: 368 , nll = 0.106487846739\n",
      "iteration: 369 , nll = 0.106092565355\n",
      "iteration: 370 , nll = 0.105700594183\n",
      "iteration: 371 , nll = 0.105309984661\n",
      "iteration: 372 , nll = 0.104922245356\n",
      "iteration: 373 , nll = 0.104531647808\n",
      "iteration: 374 , nll = 0.10415278652\n",
      "iteration: 375 , nll = 0.103766978975\n",
      "iteration: 376 , nll = 0.103390622802\n",
      "iteration: 377 , nll = 0.103011388345\n",
      "iteration: 378 , nll = 0.102638600129\n",
      "iteration: 379 , nll = 0.102268354128\n",
      "iteration: 380 , nll = 0.101896279719\n",
      "iteration: 381 , nll = 0.101533439525\n",
      "iteration: 382 , nll = 0.101169276265\n",
      "iteration: 383 , nll = 0.100810341358\n",
      "iteration: 384 , nll = 0.100446137568\n",
      "iteration: 385 , nll = 0.100089797734\n",
      "iteration: 386 , nll = 0.0997339652755\n",
      "iteration: 387 , nll = 0.0993840454254\n",
      "iteration: 388 , nll = 0.0990299796847\n",
      "iteration: 389 , nll = 0.0986839396971\n",
      "iteration: 390 , nll = 0.0983413681728\n",
      "iteration: 391 , nll = 0.0979986660114\n",
      "iteration: 392 , nll = 0.09765188601\n",
      "iteration: 393 , nll = 0.0973147466922\n",
      "iteration: 394 , nll = 0.0969800632029\n",
      "iteration: 395 , nll = 0.0966448039124\n",
      "iteration: 396 , nll = 0.0963119839689\n",
      "iteration: 397 , nll = 0.0959818298258\n",
      "iteration: 398 , nll = 0.0956541254797\n",
      "iteration: 399 , nll = 0.0953230302486\n"
     ]
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAWYAAADNCAYAAACLitqPAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nmD08PDxqDF6YPTw8PGoM/w/OFTZosKgedAAAAABJRU5ErkJggg==\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x4b5ebd0>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "#########Task I################\n",
    "res=ConvNN_Origin(400) ## just make the Making ConvNN function return the filters\n",
    "filters=res[0]\n",
    "plt.figure(1)\n",
    "for i in range(0,10):\n",
    "    plt.subplot(2,5,i), imshow(filters.get_value()[i][0],cmap=cm.gray)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Task 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#########Task II################\n",
    "def ConvNN_3_Layers(num_epochs):\n",
    "    batch_size = 250\n",
    "    learning_rate=0.1\n",
    "    nkerns=[10,10]\n",
    "\n",
    "\n",
    "    x = T.matrix('x')\n",
    "    y = T.ivector('y')\n",
    "\n",
    "    layer0_input = x.reshape((batch_size, 1, 28, 28))\n",
    "    layer0 = ConvPoolLayer(\n",
    "        input=layer0_input,\n",
    "        image_shape=(batch_size, 1, 28, 28),\n",
    "        filter_shape=(nkerns[0], 1, 5, 5),\n",
    "        poolsize=(2, 2)\n",
    "    )\n",
    "\n",
    "    layer0_output = layer0.output.flatten(2)\n",
    "    \n",
    "    layer2 = HiddenLayer(\n",
    "        input=layer0_output,\n",
    "        n_in=nkerns[1]*12*12,\n",
    "        n_out=50,\n",
    "    )\n",
    "\n",
    "    layer3 = MultiLogisticRegression(input=layer2.output, n_in=50, n_out=10)\n",
    "\n",
    "    cost = layer3.nll(y)\n",
    "\n",
    "\n",
    "    model_predict = theano.function(\n",
    "        [x],\n",
    "        layer3.predict\n",
    "    )\n",
    "\n",
    "    params = layer3.params + layer2.params + layer0.params\n",
    "    grads = T.grad(cost, params)\n",
    "    updates = [\n",
    "        (param_i, param_i - learning_rate * grad_i)\n",
    "        for param_i, grad_i in zip(params, grads)\n",
    "    ] \n",
    "\n",
    "    train_model = theano.function(\n",
    "        [x, y],\n",
    "        cost,\n",
    "        updates=updates\n",
    "    )\n",
    "    \n",
    "    dataset = 'digits.pkl.gz' \n",
    "    f = gzip.open(dataset, 'rb')\n",
    "    train_set, valid_set, test_set = cPickle.load(f)\n",
    "    f.close()\n",
    "    train_set_x, train_set_y = train_set\n",
    "    test_set_x, test_set_y = test_set\n",
    "    train_set_y = train_set_y.astype(numpy.int32)\n",
    "\n",
    "    ix = []\n",
    "    for i in range(10):\n",
    "        ix.append(numpy.nonzero(train_set_y == i)[0][:500])\n",
    "    ix = numpy.concatenate(ix)\n",
    "    train_set_x = train_set_x[ix]\n",
    "    train_set_y = train_set_y[ix]\n",
    "    ix = numpy.random.permutation(train_set_x.shape[0])\n",
    "    train_set_x = train_set_x[ix]\n",
    "    train_set_y = train_set_y[ix]\n",
    "\n",
    "    n_batches = train_set_x.shape[0]\n",
    "    n_batches /= batch_size\n",
    "    \n",
    "    n_epochs = num_epochs\n",
    "    c = numpy.zeros((n_epochs,))\n",
    "    for i in range(n_epochs): \n",
    "        err = 0\n",
    "        for b in range(n_batches):\n",
    "            train_model(train_set_x[b*batch_size:(b+1)*batch_size], train_set_y[b*batch_size:(b+1)*batch_size])\n",
    "            err += train_model(train_set_x[b*batch_size:(b+1)*batch_size], train_set_y[b*batch_size:(b+1)*batch_size])\n",
    "        print 'iteration:', i, ', nll =', err\n",
    "        c[i] = err\n",
    "  \n",
    "    n_testbatches = test_set_x.shape[0] / batch_size\n",
    "    err = 0\n",
    "    for b in range(n_testbatches):\n",
    "        yp = model_predict(test_set_x[b*batch_size:(b+1)*batch_size])\n",
    "        yy = test_set_y[b*batch_size:(b+1)*batch_size]\n",
    "        err += len(np.nonzero(yp - yy)[0])\n",
    "    return 1.0*err/len(test_set_y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def ConvNN_2_Layers(num_epochs):\n",
    "    batch_size = 250\n",
    "    learning_rate=0.1\n",
    "    nkerns=[10,10]\n",
    "\n",
    "\n",
    "    x = T.matrix('x')\n",
    "    y = T.ivector('y')\n",
    "\n",
    " \n",
    "    layer2_input = x.reshape((batch_size, 1, 28, 28)).flatten(2)\n",
    "    \n",
    "    layer2 = HiddenLayer(\n",
    "        input=layer2_input,\n",
    "        n_in=28*28,\n",
    "        n_out=50,\n",
    "    )\n",
    "\n",
    "    layer3 = MultiLogisticRegression(input=layer2.output, n_in=50, n_out=10)\n",
    "\n",
    "    cost = layer3.nll(y)\n",
    "\n",
    "\n",
    "    model_predict = theano.function(\n",
    "        [x],\n",
    "        layer3.predict\n",
    "    )\n",
    "\n",
    "    params = layer3.params + layer2.params \n",
    "    grads = T.grad(cost, params)\n",
    "    updates = [\n",
    "        (param_i, param_i - learning_rate * grad_i)\n",
    "        for param_i, grad_i in zip(params, grads)\n",
    "    ] \n",
    "\n",
    "    train_model = theano.function(\n",
    "        [x, y],\n",
    "        cost,\n",
    "        updates=updates\n",
    "    )\n",
    "    \n",
    "    dataset = 'digits.pkl.gz' \n",
    "    f = gzip.open(dataset, 'rb')\n",
    "    train_set, valid_set, test_set = cPickle.load(f)\n",
    "    f.close()\n",
    "    train_set_x, train_set_y = train_set\n",
    "    test_set_x, test_set_y = test_set\n",
    "    train_set_y = train_set_y.astype(numpy.int32)\n",
    "\n",
    "    ix = []\n",
    "    for i in range(10):\n",
    "        ix.append(numpy.nonzero(train_set_y == i)[0][:500])\n",
    "    ix = numpy.concatenate(ix)\n",
    "    train_set_x = train_set_x[ix]\n",
    "    train_set_y = train_set_y[ix]\n",
    "    ix = numpy.random.permutation(train_set_x.shape[0])\n",
    "    train_set_x = train_set_x[ix]\n",
    "    train_set_y = train_set_y[ix]\n",
    "\n",
    "    n_batches = train_set_x.shape[0]\n",
    "    n_batches /= batch_size\n",
    "    \n",
    "    n_epochs = num_epochs\n",
    "    c = numpy.zeros((n_epochs,))\n",
    "    for i in range(n_epochs): \n",
    "        err = 0\n",
    "        for b in range(n_batches):\n",
    "            train_model(train_set_x[b*batch_size:(b+1)*batch_size], train_set_y[b*batch_size:(b+1)*batch_size])\n",
    "            err += train_model(train_set_x[b*batch_size:(b+1)*batch_size], train_set_y[b*batch_size:(b+1)*batch_size])\n",
    "        print 'iteration:', i, ', nll =', err\n",
    "        c[i] = err\n",
    "  \n",
    "    n_testbatches = test_set_x.shape[0] / batch_size\n",
    "    err = 0\n",
    "    for b in range(n_testbatches):\n",
    "        yp = model_predict(test_set_x[b*batch_size:(b+1)*batch_size])\n",
    "        yy = test_set_y[b*batch_size:(b+1)*batch_size]\n",
    "        err += len(np.nonzero(yp - yy)[0])\n",
    "    return 1.0*err/len(test_set_y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "## Original CNN\n",
    "time_begin=time.time()\n",
    "res=ConvNN_Origin(400) \n",
    "time_end=time.time()\n",
    "err=res[1]\n",
    "nminutes=(time_end-time_begin)/60\n",
    "print \"The error rate of the original CNN is \"+str(err)\n",
    "print \"The time used for the original CNN is \"+str(nminutes)+\" minutes\"\n",
    "\n",
    "##1 con&pool layer, 1 hidden layer and 1 output layer\n",
    "time_begin=time.time()\n",
    "err=ConvNN_3_Layers(400)\n",
    "time_end=time.time()\n",
    "nminutes=(time_end-time_begin)/60\n",
    "print \"The error rate of 3 layers: \"+str(err)\n",
    "print \"The time used for 3 layers: \"+str(nminutes)+\" minutes\"\n",
    "\n",
    "##1 hidden layer and 1 output layer \n",
    "time_begin=time.time()\n",
    "err=ConvNN_2_Layers(400)\n",
    "time_end=time.time()\n",
    "nminutes=(time_end-time_begin)/60\n",
    "print \"The error rate of 2 layers: \"+str(err)\n",
    "print \"The time used for 2 layers: \"+str(nminutes)+\" minutes\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The error rates of these three cases are comparable. However, it is noticable that the performance is much better with the conv&pool layers."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Task 3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def ConvNN_Diff_Filters(num_epochs,filter_num0,filter_num1):\n",
    "    batch_size = 250\n",
    "    learning_rate=0.1\n",
    "    nkerns=[filter_num0, filter_num1]\n",
    "\n",
    "\n",
    "    x = T.matrix('x')\n",
    "    y = T.ivector('y')\n",
    "\n",
    "\n",
    "    layer0_input = x.reshape((batch_size, 1, 28, 28))\n",
    "    layer0 = ConvPoolLayer(\n",
    "        input=layer0_input,\n",
    "        image_shape=(batch_size, 1, 28, 28),\n",
    "        filter_shape=(nkerns[0], 1, 5, 5),\n",
    "        poolsize=(2, 2)\n",
    "    )\n",
    "\n",
    "    layer1 = ConvPoolLayer(\n",
    "        input=layer0.output,\n",
    "        image_shape=(batch_size, nkerns[0], 12, 12),\n",
    "        filter_shape=(nkerns[1], nkerns[0], 5, 5),\n",
    "        poolsize=(2, 2)\n",
    "    )  \n",
    "    layer1_output = layer1.output.flatten(2)\n",
    "\n",
    "    layer2 = HiddenLayer(\n",
    "        input=layer1_output,\n",
    "        n_in=nkerns[1]*4*4,\n",
    "        n_out=50,\n",
    "    )\n",
    "\n",
    "    layer3 = MultiLogisticRegression(input=layer2.output, n_in=50, n_out=10)\n",
    "\n",
    "    cost = layer3.nll(y)\n",
    "\n",
    "\n",
    "    model_predict = theano.function(\n",
    "        [x],\n",
    "        layer3.predict\n",
    "    )\n",
    "\n",
    "\n",
    "    params = layer3.params + layer2.params + layer1.params + layer0.params\n",
    "    grads = T.grad(cost, params)\n",
    "    updates = [\n",
    "        (param_i, param_i - learning_rate * grad_i)\n",
    "        for param_i, grad_i in zip(params, grads)\n",
    "    ] \n",
    "\n",
    "    train_model = theano.function(\n",
    "        [x, y],\n",
    "        cost,\n",
    "        updates=updates\n",
    "    )\n",
    "    \n",
    "    dataset = 'digits.pkl.gz' \n",
    "    f = gzip.open(dataset, 'rb')\n",
    "    train_set, valid_set, test_set = cPickle.load(f)\n",
    "    f.close()\n",
    "    train_set_x, train_set_y = train_set\n",
    "    test_set_x, test_set_y = test_set\n",
    "    train_set_y = train_set_y.astype(numpy.int32)\n",
    "\n",
    "    ix = []\n",
    "    for i in range(10):\n",
    "        ix.append(numpy.nonzero(train_set_y == i)[0][:500])\n",
    "    ix = numpy.concatenate(ix)\n",
    "    train_set_x = train_set_x[ix]\n",
    "    train_set_y = train_set_y[ix]\n",
    "    ix = numpy.random.permutation(train_set_x.shape[0])\n",
    "    train_set_x = train_set_x[ix]\n",
    "    train_set_y = train_set_y[ix]\n",
    "\n",
    "    n_batches = train_set_x.shape[0]\n",
    "    n_batches /= batch_size\n",
    "    \n",
    "    n_epochs = num_epochs\n",
    "    c = numpy.zeros((n_epochs,))\n",
    "    for i in range(n_epochs): \n",
    "        err = 0\n",
    "        for b in range(n_batches):\n",
    "            train_model(train_set_x[b*batch_size:(b+1)*batch_size], train_set_y[b*batch_size:(b+1)*batch_size])\n",
    "            err += train_model(train_set_x[b*batch_size:(b+1)*batch_size], train_set_y[b*batch_size:(b+1)*batch_size])\n",
    "        print 'iteration:', i, ', nll =', err\n",
    "        c[i] = err\n",
    "  \n",
    "    n_testbatches = test_set_x.shape[0] / batch_size\n",
    "    err = 0\n",
    "    for b in range(n_testbatches):\n",
    "        yp = model_predict(test_set_x[b*batch_size:(b+1)*batch_size])\n",
    "        yy = test_set_y[b*batch_size:(b+1)*batch_size]\n",
    "        err += len(np.nonzero(yp - yy)[0])\n",
    "    return 1.0*err/len(test_set_y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "iteration: 0 , nll = 43.3408711644\n",
      "iteration: 1 , nll = 34.2469178779\n",
      "iteration: 2 , nll = 25.1754257657\n",
      "iteration: 3 , nll = 19.2491573177\n",
      "iteration: 4 , nll = 15.5170431597\n",
      "iteration: 5 , nll = 13.0163654501\n",
      "iteration: 6 , nll = 11.2326203874\n",
      "iteration: 7 , nll = 9.89982827761\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-9-11041425e69a>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[1;31m#10 filters for layer1 and 20 filters for layer2\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      2\u001b[0m \u001b[0mtime_begin\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mtime\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtime\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 3\u001b[1;33m \u001b[0merr\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mConvNN_Diff_Filters\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;36m400\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;36m10\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;36m20\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      4\u001b[0m \u001b[0mtime_end\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mtime\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtime\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      5\u001b[0m \u001b[0mnminutes\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtime_end\u001b[0m\u001b[1;33m-\u001b[0m\u001b[0mtime_begin\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m/\u001b[0m\u001b[1;36m60\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m<ipython-input-8-a7efa7d4267a>\u001b[0m in \u001b[0;36mConvNN_Diff_Filters\u001b[1;34m(num_epochs, filter_num0, filter_num1)\u001b[0m\n\u001b[0;32m     81\u001b[0m         \u001b[0merr\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;36m0\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     82\u001b[0m         \u001b[1;32mfor\u001b[0m \u001b[0mb\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mn_batches\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 83\u001b[1;33m             \u001b[0mtrain_model\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtrain_set_x\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mb\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0mbatch_size\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mb\u001b[0m\u001b[1;33m+\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0mbatch_size\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtrain_set_y\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mb\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0mbatch_size\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mb\u001b[0m\u001b[1;33m+\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0mbatch_size\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     84\u001b[0m             \u001b[0merr\u001b[0m \u001b[1;33m+=\u001b[0m \u001b[0mtrain_model\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtrain_set_x\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mb\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0mbatch_size\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mb\u001b[0m\u001b[1;33m+\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0mbatch_size\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtrain_set_y\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mb\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0mbatch_size\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mb\u001b[0m\u001b[1;33m+\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0mbatch_size\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     85\u001b[0m         \u001b[1;32mprint\u001b[0m \u001b[1;34m'iteration:'\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mi\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m', nll ='\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0merr\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m/usr/local/lib/python2.7/dist-packages/theano/compile/function_module.pyc\u001b[0m in \u001b[0;36m__call__\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m    593\u001b[0m         \u001b[0mt0_fn\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtime\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtime\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    594\u001b[0m         \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 595\u001b[1;33m             \u001b[0moutputs\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfn\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    596\u001b[0m         \u001b[1;32mexcept\u001b[0m \u001b[0mException\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    597\u001b[0m             \u001b[1;32mif\u001b[0m \u001b[0mhasattr\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfn\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m'position_of_error'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "#10 filters for layer1 and 20 filters for layer2\n",
    "time_begin=time.time()\n",
    "err=ConvNN_Diff_Filters(400,10,20)\n",
    "time_end=time.time()\n",
    "nminutes=(time_end-time_begin)/60\n",
    "print \"The error rate of 10 filters for layer1 and 20 filters for layer2: \"+str(err)\n",
    "print \"The time used for 10 filters for layer1 and 20 filters for layer2: \"+str(nminutes)+\" minutes\"\n",
    "\n",
    "#20 filters for layer1 and 10 filters for layer2\n",
    "time_begin=time.time()\n",
    "ConvNN_Diff_Filters(400,20,10)\n",
    "time_end=time.time()\n",
    "nminutes=(time_end-time_begin)/60\n",
    "print \"The error rate of 20 filters for layer1 and 10 filters for layer2: \"+str(err)\n",
    "print \"The time used for 20 filters for layer1 and 10 filters for layer2: \"+str(nminutes)+\" minutes\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "With the iteration number 1200, the error rates of these two cases are the same as that of the original. However, more filters will leader to significantly worse performance. "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
